# üèóÔ∏è Arquitetura T√©cnica - DataMaster SentimentalReview

![Arquitetura Flow](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExam04ZzFuYzZlM3cyMTQ3ZjRxZmU1bGhhcjh1d2s2ZGZ5OXViYzNkMSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3oEduMkS4VLz7x8eYg/giphy.gif)

Este documento detalha as decis√µes arquiteturais, trade-offs e implementa√ß√£o t√©cnica do projeto DataMaster SentimentalReview. Eu projetei esta arquitetura para demonstrar expertise em engenharia de dados moderna, combinando Data Lakehouse, processamento distribu√≠do e an√°lise de sentimento.

---

## üìã √çndice

- [üéØ Vis√£o Geral da Arquitetura](#-vis√£o-geral-da-arquitetura)
- [üèõÔ∏è Arquitetura Medalh√£o](#Ô∏è-arquitetura-medalh√£o)
- [üîÑ Orquestra√ß√£o com Airflow](#-orquestra√ß√£o-com-airflow)
- [‚ö° Processamento com Spark](#-processamento-com-spark)
- [üíæ Armazenamento e Cat√°logo](#-armazenamento-e-cat√°logo)
- [üîç Query Engine e Visualiza√ß√£o](#-query-engine-e-visualiza√ß√£o)
- [üì° Streaming e Ingest√£o](#-streaming-e-ingest√£o)
- [üéØ Trade-offs e Decis√µes](#-trade-offs-e-decis√µes)

---

## üéØ Vis√£o Geral da Arquitetura

### Princ√≠pios Arquiteturais

**1. Data Lakehouse H√≠brido**
- Combina flexibilidade de Data Lake com performance de Data Warehouse
- ACID transactions via Delta Lake
- Schema evolution autom√°tica
- Time travel para auditoria

**2. Processamento Distribu√≠do**
- Apache Spark para transforma√ß√µes pesadas
- Paraleliza√ß√£o autom√°tica via particionamento
- Otimiza√ß√µes AQE (Adaptive Query Execution)

**3. Orquestra√ß√£o Declarativa**
- Apache Airflow com DAGs complexas
- Depend√™ncias expl√≠citas entre tasks
- Retry policies e alertas autom√°ticos

### Diagrama de Componentes

```mermaid
graph TB
    subgraph "Camada de Ingest√£o"
        API[Google Maps API]
        MOCK[Mock JSON Server]
        KAFKA[Apache Kafka]
    end
    
    subgraph "Camada de Orquestra√ß√£o"
        AIRFLOW[Apache Airflow]
        SCHEDULER[Airflow Scheduler]
        WEBSERVER[Airflow Webserver]
    end
    
    subgraph "Camada de Processamento"
        SPARK_MASTER[Spark Master]
        SPARK_WORKER[Spark Worker]
        JOBS[PySpark Jobs]
    end
    
    subgraph "Data Lakehouse (MinIO S3)"
        LANDING[Landing Zone<br/>Raw JSON]
        BRONZE[Bronze Layer<br/>Structured Delta]
        SILVER[Silver Layer<br/>Enriched + NLP]
        GOLD[Gold Layer<br/>Business Metrics]
    end
    
    subgraph "Camada de Consulta"
        TRINO[Apache Trino]
        HIVE[Hive Metastore]
        MYSQL[MySQL Backend]
    end
    
    subgraph "Camada de Visualiza√ß√£o"
        SUPERSET[Apache Superset]
        POSTGRES[PostgreSQL Backend]
    end
    
    API --> KAFKA
    MOCK --> KAFKA
    KAFKA --> AIRFLOW
    AIRFLOW --> SPARK_MASTER
    SPARK_MASTER --> SPARK_WORKER
    SPARK_WORKER --> JOBS
    JOBS --> LANDING
    LANDING --> BRONZE
    BRONZE --> SILVER
    SILVER --> GOLD
    HIVE --> MYSQL
    TRINO --> HIVE
    SUPERSET --> TRINO
    SUPERSET --> POSTGRES
```

---

## üèõÔ∏è Arquitetura Medalh√£o

![Medallion Architecture](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExOTF2bnBqbmhmZ3N1ZGl0MXNteXM2dTkzaTEybGs3bTQyMmM5d2Y5aCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/26uf8bAO8Fk0e88Io/giphy.gif)

### Landing Zone (Camada Bronze)
**Prop√≥sito:** Ingest√£o de dados brutos sem transforma√ß√£o

**Caracter√≠sticas:**
- Formato: JSON original da API
- Particionamento: `year=YYYY/month=MM/day=DD/bairro=nome/`
- Reten√ß√£o: 90 dias (configur√°vel)
- Schema: Schema-on-read

**Exemplo de estrutura:**
```
s3a://datalake/landing/google_maps/
‚îú‚îÄ‚îÄ year=2025/month=08/day=30/
‚îÇ   ‚îú‚îÄ‚îÄ bairro=vila_madalena/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_id=20250830_143000/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ agencias.json
‚îÇ   ‚îî‚îÄ‚îÄ bairro=pinheiros/
‚îÇ       ‚îî‚îÄ‚îÄ run_id=20250830_143000/
‚îÇ           ‚îî‚îÄ‚îÄ agencias.json
```

### Bronze Layer (Dados Estruturados)
**Prop√≥sito:** Dados limpos e estruturados em formato Delta

**Transforma√ß√µes aplicadas:**
- Normaliza√ß√£o de schema
- Deduplica√ß√£o por `place_id`
- Valida√ß√£o de tipos de dados
- Expans√£o de arrays aninhados (reviews)

**Schema Bronze:**
```sql
CREATE TABLE bronze.avaliacoes (
    place_id STRING,
    nome_agencia STRING,
    endereco_completo STRING,
    latitude DOUBLE,
    longitude DOUBLE,
    rating DOUBLE,
    total_avaliacoes INT,
    review_id STRING,
    review_texto STRING,
    review_rating INT,
    review_data TIMESTAMP,
    bairro_origem STRING,
    data_coleta DATE,
    data_processamento TIMESTAMP
) USING DELTA
PARTITIONED BY (data_coleta, bairro_origem)
```

### Silver Layer (Dados Enriquecidos)
**Prop√≥sito:** Dados com an√°lise de sentimento e detec√ß√£o de PII

**Enriquecimentos aplicados:**
- **An√°lise de Sentimento:** Dicion√°rio de palavras positivas/negativas em portugu√™s brasileiro
- **Detec√ß√£o de PII:** Regex patterns para CPF, telefone, email e outros dados sens√≠veis
- **An√°lise de Risco:** Keywords de problemas reputacionais
- **Normaliza√ß√£o de Texto:** Limpeza e padroniza√ß√£o

**Schema Silver:**
```sql
CREATE TABLE silver.avaliacoes_enriquecidas (
    -- Campos originais da Bronze
    place_id STRING,
    nome_agencia STRING,
    review_texto STRING,
    review_rating INT,
    
    -- Campos de NLP
    sentimento STRING,  -- POSITIVO, NEGATIVO, NEUTRO
    sentimento_score DOUBLE,  -- Confian√ßa do modelo
    texto_anonimizado STRING,  -- Texto com PII mascarado
    pii_detectado ARRAY<STRING>,  -- Tipos de PII encontrados
    risco_reputacional_detectado BOOLEAN,
    categorias_risco ARRAY<STRING>,  -- Tipos de problemas
    
    -- Metadados de processamento
    modelo_sentimento STRING,
    versao_pipeline STRING,
    data_processamento_nlp TIMESTAMP
) USING DELTA
PARTITIONED BY (data_coleta, sentimento)
```

### Gold Layer (M√©tricas de Neg√≥cio)
**Prop√≥sito:** Agrega√ß√µes e KPIs para dashboards

**Tabelas Gold implementadas:**

1. **`gold.kpis_agencias_diario`**
```sql
CREATE TABLE gold.kpis_agencias_diario (
    place_id STRING,
    nome_agencia STRING,
    bairro STRING,
    data_referencia DATE,
    
    -- M√©tricas de Volume
    total_avaliacoes INT,
    avaliacoes_periodo INT,
    
    -- M√©tricas de Sentimento
    rating_medio DOUBLE,
    sentimento_positivo_pct DOUBLE,
    sentimento_negativo_pct DOUBLE,
    sentimento_neutro_pct DOUBLE,
    
    -- M√©tricas de Risco
    avaliacoes_risco_alto INT,
    principais_problemas ARRAY<STRING>,
    
    -- Metadados
    data_atualizacao TIMESTAMP
) USING DELTA
PARTITIONED BY (data_referencia)
```

2. **`gold.tendencias_sentimento`**
```sql
CREATE TABLE gold.tendencias_sentimento (
    bairro STRING,
    data_referencia DATE,
    sentimento STRING,
    
    -- M√©tricas Agregadas
    total_avaliacoes INT,
    rating_medio DOUBLE,
    score_sentimento_medio DOUBLE,
    
    -- Comparativo
    variacao_7d DOUBLE,
    variacao_30d DOUBLE,
    
    data_atualizacao TIMESTAMP
) USING DELTA
PARTITIONED BY (data_referencia, bairro)
```

---

## üîÑ Orquestra√ß√£o com Airflow

![Workflow Orchestration](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExMzduczk5aDJrN3lmYmVsZ2V5dXljbm1yZTYyYTN2bjQ1ajd3ZWs4NSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/CNTQMX7Xodf5aZqUGf/giphy.gif)

**Interface Web:** http://localhost:8089 (admin/admin)
**Backend:** PostgreSQL para metadados
**Executor:** LocalExecutor

### DAG Principal: [`dag_coleta_google_maps`](../mnt/airflow/dags/dag_coleta_google_maps.py)

**Caracter√≠sticas:**
- Schedule: Manual (para demonstra√ß√£o)
- Retries: 2 tentativas com backoff exponencial
- SLA: 30 minutos por task
- Alertas: Email em falhas

**Fluxo de Tasks:**

```mermaid
graph LR
    A[verificar_quota] --> B[obter_bairros_pendentes]
    B --> C[coletar_dados_google_maps]
    C --> D[salvar_dados]
    D --> E[verificar_qualidade]
    E --> F[verificar_integridade]
    F --> G[gerar_relatorio]
```

**Tasks Detalhadas:**

1. **`verificar_quota_google_maps`**
   - Verifica quota dispon√≠vel da API
   - Cancela execu√ß√£o se < 100 requests
   - Implementa rate limiting inteligente

2. **`obter_bairros_pendentes`**
   - Consulta controle incremental
   - Retorna pr√≥ximo bairro a processar
   - Evita reprocessamento desnecess√°rio

3. **`coletar_dados_google_maps`**
   - Executa busca por ag√™ncias Santander
   - Aplica filtros por tipo e localiza√ß√£o
   - Coleta reviews detalhados

4. **`salvar_dados`**
   - Upload para MinIO (Landing Zone)
   - Salvamento local para backup
   - Particionamento autom√°tico

5. **`verificar_qualidade_dados`**
   - Valida√ß√£o de schema
   - Detec√ß√£o de duplicatas
   - M√©tricas de completude

### Controle Incremental

**Arquivo:** [`mnt/airflow/scripts/controle_incremental.py`](../mnt/airflow/scripts/controle_incremental.py)

**Funcionalidades:**
- Controle sequencial por bairros
- Persist√™ncia entre execu√ß√µes
- Avan√ßo autom√°tico ap√≥s esgotamento
- Estat√≠sticas de progresso

**Exemplo de Estado:**
```json
{
  "bairro_atual_index": 1,
  "data_ultima_atualizacao": "2025-08-30T14:40:00",
  "bairros_processados": {
    "Vila Madalena": {
      "agencias_consultadas": ["ChIJ123", "ChIJ456"],
      "total_agencias_encontradas": 3,
      "data_ultima_consulta": "2025-08-30T14:30:00"
    }
  },
  "estatisticas": {
    "total_bairros": 52,
    "total_agencias_processadas": 5,
    "total_comentarios_coletados": 120
  }
}
```

---

## ‚ö° Processamento com Spark

![Distributed Computing](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExb3ZjbGVuMWtwcWE0azYzaXdpNWxmZGR6bXdhN3Bxc3FzNzEzcGs1bSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/jfxHI6u1E15eHTY8rb/giphy.gif)

**Interface Web:** http://localhost:8081 (Spark Master UI)
**Master URL:** spark://spark-master:7077
**Configura√ß√£o:** 1 Master + 1 Worker (512MB RAM cada)

### Jobs PySpark Implementados

**1. Landing to Bronze (`landing_to_bronze.py`)**
```python
def create_spark_session():
    return SparkSession.builder \
        .appName("DataMaster_LandingToBronze") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()
```

**Transforma√ß√µes aplicadas:**
- Leitura de JSONs da Landing Zone
- Normaliza√ß√£o de schema
- Deduplica√ß√£o por `place_id`
- Expans√£o de arrays de reviews
- Salvamento em formato Delta

**2. Bronze to Silver (`bronze_to_silver_fixed.py`)**

**An√°lise de Sentimento:**
```python
# UDFs tradicionais do Spark (n√£o pandas_udf)
def sentiment_analysis_ptbr_udf():
    """UDF para an√°lise de sentimento em portugu√™s brasileiro"""
    
    def analyze_sentiment(text):
        if text is None or text.strip() == "":
            return "neutro"
        
        text_lower = text.lower()
        
        positivas_count = len([palavra for palavra in palavras_positivas if palavra in text_lower])
        negativas_count = len([palavra for palavra in palavras_negativas if palavra in text_lower])
        
        if positivas_count > negativas_count:
            return "positivo"
        elif negativas_count > positivas_count:
            return "negativo"
        else:
            return "neutro"
    
    return udf(analyze_sentiment, StringType())

def detect_pii_udf():
    """UDF para detec√ß√£o de PII usando regex patterns"""
    # Implementa√ß√£o com dicion√°rios de sentimento + regex PII
    # Abordagem dictionary-based para transpar√™ncia e performance
    
    palavras_positivas = {
        'excelente', '√≥timo', 'bom', 'maravilhoso', 'fant√°stico',
        'perfeito', 'incr√≠vel', 'satisfeito', 'feliz', 'recomendo'
    }
    
    palavras_negativas = {
        'p√©ssimo', 'horr√≠vel', 'ruim', 'terr√≠vel', 'desastroso',
        'lament√°vel', 'decepcionante', 'insatisfeito', 'n√£o recomendo'
    }
    
    # An√°lise de sentimento baseada em contagem de palavras
    # Detec√ß√£o PII com regex patterns para CPF, telefone, email
```

**3. Silver to Gold (`silver_to_gold_working.py`)**

**Agrega√ß√µes implementadas:**
- KPIs por ag√™ncia/dia
- Tend√™ncias de sentimento
- M√©tricas de risco reputacional
- Comparativos temporais

### Otimiza√ß√µes Spark

**Configura√ß√µes de Performance:**
```python
# Adaptive Query Execution
"spark.sql.adaptive.enabled": "true"
"spark.sql.adaptive.coalescePartitions.enabled": "true"

# Otimiza√ß√µes Delta Lake
"spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog"

# Configura√ß√µes MinIO
"spark.hadoop.fs.s3a.path.style.access": "true"
"spark.hadoop.fs.s3a.connection.ssl.enabled": "false"
```

**Particionamento Inteligente:**
- Bronze: Por `data_coleta` e `bairro_origem`
- Silver: Por `data_coleta` e `sentimento`
- Gold: Por `data_referencia`

---

## üíæ Armazenamento e Cat√°logo

![Data Storage](https://media.giphy.com/media/26BRBKqUiq586bRVm/giphy.gif)

### MinIO S3-Compatible Storage

**Configura√ß√£o:**
- Endpoint: `http://minio:9000`
- Interface Web: http://localhost:9000 (minioadmin/minioadmin123)
- Credenciais API: `minio/minio123` ([`docker-compose.yml`](../docker-compose.yml))
- Bucket √∫nico: `datalake`
- Pol√≠tica: Acesso p√∫blico para leitura

**Estrutura de Diret√≥rios:**
```
datalake/
‚îú‚îÄ‚îÄ landing/google_maps/year=2025/month=08/day=30/
‚îú‚îÄ‚îÄ bronze/avaliacoes/data_coleta=2025-08-30/
‚îú‚îÄ‚îÄ silver/avaliacoes_enriquecidas/data_coleta=2025-08-30/
‚îî‚îÄ‚îÄ gold/
    ‚îú‚îÄ‚îÄ kpis_agencias_diario/data_referencia=2025-08-30/
    ‚îî‚îÄ‚îÄ tendencias_sentimento/data_referencia=2025-08-30/
```

### Hive Metastore

**Configura√ß√£o:**
- Backend: MariaDB 10.5.16 (porta 3306)
- Porta: 9083
- Integra√ß√£o: Trino + Spark
- Credenciais: admin/admin

**Cat√°logos configurados:**
- `hive`: Tabelas tradicionais
- `delta`: Tabelas Delta Lake
- `memory`: Tabelas tempor√°rias

---

## üîç Query Engine e Visualiza√ß√£o

![Query Processing](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExdXJkMTQ1a20xOWN3Ynl0eDh6dWpoc3BuM3VlYjhycnpyc3Vwc2dwNCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/xT9C25UNTwfZuk85WP/giphy.gif)

### Apache Trino

**Conectores habilitados:**
- `hive`: Acesso ao Hive Metastore
- `delta`: Tabelas Delta Lake nativas
- `memory`: Consultas ad-hoc

**Configura√ß√£o S3:**
```properties
hive.s3.endpoint=http://minio:9000
hive.s3.path-style-access=true
hive.s3.ssl.enabled=false
```

### Apache Superset

**Integra√ß√£o:**
- Driver: `trino://admin@trino:8080/hive/default`
- Interface Web: http://localhost:8088 (admin/admin123)
- Autentica√ß√£o: Basic Auth
- Timeout: 300s para queries pesadas

**Dashboards implementados:**
- Vis√£o geral de sentimento por bairro
- Tend√™ncias temporais de satisfa√ß√£o
- Alertas de risco reputacional
- M√©tricas de qualidade de dados

---

## üì° Streaming e Ingest√£o

![Data Streaming](https://media.giphy.com/media/l0HlR2eV8koAztu2Q/giphy.gif)

### Apache Kafka

**Configura√ß√£o:**
- Porta externa: 9092 (localhost)
- Porta interna: 29092 (kafka)
- Zookeeper: porta 2181
- Imagem: Confluent CP-Kafka 7.4.0

**T√≥picos configurados:**
- `bairros_sp`: Dados de bairros para processamento
- `google_maps_raw`: Dados brutos da API Google Maps
- `avaliacoes_processadas`: Dados processados do pipeline
**Configura√ß√£o de T√≥picos:**
- Parti√ß√µes: 1 por t√≥pico (ambiente local)
- Replication Factor: 1 (ambiente local)
- Retention: 7 dias

### Mock Google Maps API

**Implementa√ß√£o:** JSON Server local (db.json)
**Porta:** N√£o exposta (dados est√°ticos)

**Endpoints simulados:**
- `textsearch`: 50+ ag√™ncias Santander em SP
- `place_details_*`: Detalhes com reviews sint√©ticas
- Dados realistas com CPFs, ratings, coordenadas GPS

**Dados simulados:**
- 52 bairros de S√£o Paulo
- ~200 ag√™ncias Santander
- ~2000 reviews sint√©ticas

---

## üéØ Trade-offs e Decis√µes

![Technical Decisions](https://media.giphy.com/media/26BRuo6sLetdllPAQ/giphy.gif)

![Data Architecture](https://media.giphy.com/media/3oKIPEqDGUULpEU0aQ/giphy.gif)

### Por que Delta Lake?

**‚úÖ Vantagens:**
- ACID transactions nativas
- Time travel para auditoria
- Schema evolution autom√°tica
- Melhor integra√ß√£o com Spark

**‚ùå Desvantagens:**
- Vendor lock-in (Databricks)
- Menos maduro que Parquet
- Overhead de metadados

**Alternativa considerada:** Apache Iceberg
- Mais vendor-neutral
- Melhor para multi-engine
- Escolhi Delta pela simplicidade

### Por que MinIO local?

**‚úÖ Vantagens:**
- Zero custo operacional
- S3-compatible API
- Controle total dos dados
- Ideal para demonstra√ß√£o

**‚ùå Desvantagens:**
- N√£o √© distribu√≠do
- Limitado por hardware local
- Sem features enterprise (encryption, etc.)

### Por que Trino?

**‚úÖ Vantagens:**
- Performance superior ao Presto
- Conectores nativos Delta/Iceberg
- Integra√ß√£o perfeita com Superset
- Query federation

**‚ùå Desvantagens:**
- Complexidade de configura√ß√£o
- Consumo de mem√≥ria alto
- Curva de aprendizado

---

## üìñ Di√°rio de Bordo

![Engineering Excellence](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExbXMybXB1dTF0aW5iaWxqYngxazQ2N2JraTU2cWNxaHgycTU1bzFqcyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/QBDBmFhiVrZcwW79j9/giphy.gif)

### Decis√µes que me orgulho

1. **Controle incremental por bairros:** Evita reprocessamento e permite paraleliza√ß√£o futura
2. **UDFs tradicionais para NLP:** Abordagem dictionary-based transparente e eficiente
3. **Configura√ß√µes otimizadas:** 5-6GB RAM vs 12GB+ de setups tradicionais
4. **Mock API realista:** Dados sint√©ticos mas estrutura real da Google Maps API

### O que eu mudaria hoje

1. **Kubernetes desde o in√≠cio:** Docker Compose tem limita√ß√µes de escala
2. **Observabilidade nativa:** Prometheus + Grafana integrados
3. **Testes automatizados:** PyTest para jobs Spark desde o in√≠cio
4. **Infraestrutura como c√≥digo:** Terraform para reprodutibilidade

### Li√ß√µes aprendidas

- **Health checks s√£o cr√≠ticos:** Evitam falhas em cascata
- **Particionamento importa:** Diferen√ßa entre segundos e minutos
- **Documenta√ß√£o paga dividendos:** Facilita manuten√ß√£o e onboarding
- **Configura√ß√£o via [`.env`](../.env):** Flexibilidade entre ambientes

---

## üìä M√©tricas de Performance

![Performance Metrics](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExMjlsNHg1NDVsbjNsdDNuMjM2bTgyeDBzOGNzNXNic2Q4bXVoeWNmcSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/1Bo84Z4ptN78iOlcN2/giphy.gif)

### Benchmarks Locais

**Hardware de refer√™ncia:**
- CPU: 8 cores
- RAM: 16GB
- Storage: SSD NVMe

### Escalabilidade Projetada

**Para produ√ß√£o (AWS/GCP):**
- Spark cluster: 10-50 workers
- MinIO ‚Üí S3/GCS nativo
- Trino cluster: 3-10 nodes
- Performance esperada: 10-100x

---

## üîÑ Pr√≥ximas Evolu√ß√µes

### Melhorias Arquiteturais

1. **Real-time Streaming:** Kafka Streams + ksqlDB
2. **ML Pipeline:** MLflow para modelos NLP
3. **Data Quality:** Great Expectations framework
4. **Observabilidade:** Prometheus + Grafana + Jaeger

### Migra√ß√£o para Cloud

**AWS Stack equivalente:**
- MinIO ‚Üí S3
- Spark ‚Üí EMR/Glue
- Trino ‚Üí Athena/Redshift
- Airflow ‚Üí MWAA
- Superset ‚Üí QuickSight



---

**Resultados**

Esta arquitetura combina conceitos avan√ßados de engenharia de dados:
- ‚úÖ Data Lakehouse moderno
- ‚úÖ Processamento distribu√≠do otimizado  
- ‚úÖ Orquestra√ß√£o robusta
- ‚úÖ An√°lise de sentimento em produ√ß√£o
- ‚úÖ Configura√ß√£o enterprise em ambiente local

√â uma implementa√ß√£o que funcionaria em produ√ß√£o com ajustes m√≠nimos.

---

*Documenta√ß√£o t√©cnica por [Danilo Vruck](mailto:danilo.vruck@email.com)*
