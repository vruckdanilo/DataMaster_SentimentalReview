# ‚ùì FAQ e Troubleshooting - DataMaster SentimentalReview

![Troubleshooting](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExZGprYzJkYXl3a3RxNG85MDhxa3p4ZTV0bnZzenRoMTdqdm5rcG03dyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/7K3p2z8Hh9QOI/giphy.gif)

Este documento consolida as perguntas mais frequentes e solu√ß√µes para problemas comuns encontrados durante instala√ß√£o, configura√ß√£o e opera√ß√£o do sistema.

---

## üìã √çndice

- [üöÄ Instala√ß√£o e Setup](#-instala√ß√£o-e-setup)
- [üê≥ Problemas com Docker](#-problemas-com-docker)
- [‚ö° Issues do Spark](#-issues-do-spark)
- [üîÑ Problemas do Airflow](#-problemas-do-airflow)
- [üìä Superset e Visualiza√ß√£o](#-superset-e-visualiza√ß√£o)
- [üîß Performance e Recursos](#-performance-e-recursos)
- [üìÅ Dados e Pipeline](#-dados-e-pipeline)

---

## üöÄ Instala√ß√£o e Setup

![Setup Process](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExbGdyam40ZTRqbDVhM3N6azh0ZWt2cDJoemtndXF0NGpxZnlhcnFuayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/AqV8uSb8ptxyo7Wyog/giphy.gif)

### Q: O script start.sh falha com "Permission denied"
**A:** Torne o script execut√°vel:
```bash
chmod +x [`scripts/start.sh`](../scripts/start.sh)
[`./scripts/start.sh`](../scripts/start.sh)
```

### Q: Erro "Port already in use" durante inicializa√ß√£o
**A:** Verifique processos usando as portas:
```bash
# Verificar portas ocupadas
sudo lsof -i :8089  # Airflow
sudo lsof -i :8088  # Superset
sudo lsof -i :8080  # Trino
sudo lsof -i :8082  # Spark Master
sudo lsof -i :8081  # Spark Worker

# Parar processos conflitantes
sudo systemctl stop apache2
sudo systemctl stop nginx

# Ou matar processo espec√≠fico
sudo kill -9 $(sudo lsof -t -i:8089)
sudo kill -9 $(sudo lsof -t -i:8088)
sudo kill -9 $(sudo lsof -t -i:8080)
```

### Q: Containers ficam "unhealthy" ap√≥s inicializa√ß√£o
**A:** Aguarde mais tempo (primeira inicializa√ß√£o pode levar 10-15 min):
```bash
# Verificar logs espec√≠ficos
docker compose logs -f [service-name]

# Reiniciar servi√ßo espec√≠fico
docker compose restart [service-name]

# Verificar recursos dispon√≠veis
docker stats
```

### Q: Erro "No space left on device"
**A:** Limpe imagens e volumes n√£o utilizados:
```bash
# Limpar sistema Docker
docker system prune -f

# Limpar volumes √≥rf√£os
docker volume prune -f

# Verificar espa√ßo
df -h
docker system df
```

---

## üê≥ Problemas com Docker

![Docker Issues](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExcXZwOHhocWtycHk1bzZ1bDJwaDZ0NGYzZnJwdGtrczR5Z2c1OTlicCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/xVurSXSjNiifP8qLXy/giphy.gif)

### Q: "Cannot connect to Docker daemon"
**A:** Verifique se Docker est√° rodando:
```bash
# Verificar status
sudo systemctl status docker

# Iniciar Docker
sudo systemctl start docker

# Adicionar usu√°rio ao grupo docker
sudo usermod -aG docker $USER
newgrp docker
```

### Q: Containers crasham por falta de mem√≥ria
**A:** Aumente recursos no Docker Desktop:
```bash
# Docker Desktop: Settings > Resources > Memory > 8GB+

# Ou reduza configura√ß√µes no [`.env`](../.env)
SPARK_DRIVER_MEMORY=256m
SPARK_EXECUTOR_MEMORY=256m
# (Valores atuais: 512m cada)
```

### Q: Build das imagens falha
**A:** Limpe cache e rebuilde:
```bash
# Limpar cache de build
docker builder prune -f

# Rebuild for√ßado
docker compose build --no-cache

# Verificar espa√ßo em disco
df -h
```

### Q: Rede Docker com problemas de conectividade
**A:** Recrie a rede:
```bash
# Parar containers
docker compose down

# Remover rede
docker network rm datamaster_sentimentalreview_data_network

# Recriar tudo
docker compose up -d
```

---

## ‚ö° Issues do Spark



### Q: Spark jobs falham com "Error code is: 1"
**A:** Verifique mapeamento de volumes e caminhos:
```bash
# Verificar se arquivos existem no container
docker compose exec spark-master ls -la /opt/bitnami/spark/jobs/

# Verificar logs detalhados
docker compose logs spark-master | grep -i error

# Testar job simples
docker compose exec spark-master spark-submit --version
```

### Q: "java.net.UnknownHostException: hive-metastore"
**A:** Use HadoopCatalog em vez de HiveCatalog:
```python
# Em jobs Spark, configure:
.config("spark.sql.catalog.lakehouse.type", "hadoop")
# Remover: .config("spark.sql.catalog.lakehouse.uri", "thrift://hive-metastore:9083")
```

### Q: Spark Thrift Server n√£o inicia na porta 10000
**A:** Verifique configura√ß√µes e depend√™ncias:
```bash
# Verificar se porta est√° aberta
nc -zv localhost 10000

# Verificar logs do Thrift Server
docker compose logs spark-master | grep -i thrift

# Testar conectividade interna
docker compose exec superset ping spark-master
```

### Q: "OutOfMemoryError" em jobs Spark
**A:** Ajuste configura√ß√µes de mem√≥ria:
```bash
# No [`.env`](../.env), aumente:
SPARK_DRIVER_MEMORY=1g
SPARK_EXECUTOR_MEMORY=1g
# (Valores atuais: 512m cada)

# Ou no job Spark:
.config("spark.driver.memory", "1g")
.config("spark.executor.memory", "1g")
```

---

## üîÑ Problemas do Airflow



### Q: DAGs n√£o aparecem na interface
**A:** Verifique diret√≥rio e permiss√µes:
```bash
# Verificar arquivos DAG
docker compose exec airflow-webserver ls -la /opt/airflow/dags/

# Verificar logs do scheduler
docker compose logs airflow-scheduler | grep -i dag

# For√ßar refresh
docker compose exec airflow-webserver airflow dags list-import-errors
```

### Q: "No module named 'scripts'" em DAGs
**A:** Verifique mapeamento de volumes e __init__.py:
```bash
# Verificar mapeamento
docker compose exec airflow-webserver ls -la /opt/airflow/scripts/

# Criar __init__.py se n√£o existir
touch [`mnt/airflow/scripts/__init__.py`](../mnt/airflow/scripts/__init__.py)

# Reiniciar scheduler
docker compose restart airflow-scheduler
```

### Q: Tasks ficam em estado "running" indefinidamente
**A:** Verifique recursos e reinicie:
```bash
# Verificar tasks ativas
docker compose exec airflow-webserver airflow tasks list dag_id

# Matar tasks travadas
docker compose exec airflow-webserver airflow tasks clear dag_id

# Reiniciar workers
docker compose restart airflow-scheduler
```

### Q: Conex√µes com outros servi√ßos falham
**A:** Use nomes de servi√ßos Docker, n√£o localhost:
```python
# ‚úÖ Correto
MINIO_ENDPOINT = "minio:9000"
KAFKA_BOOTSTRAP_SERVERS = "kafka:29092"

# ‚ùå Incorreto
MINIO_ENDPOINT = "localhost:9000"
KAFKA_BOOTSTRAP_SERVERS = "localhost:9092"
```

---

## üìä Superset e Visualiza√ß√£o



### Q: Erro 405 "Method Not Allowed" no Superset
**A:** Desabilite CSRF temporariamente:
```python
# Em superset_config.py
WTF_CSRF_ENABLED = False
ENABLE_PROXY_FIX = True
```

### Q: "Could not connect to any of [('spark-master', 10000)]"
**A:** Verifique conectividade e configura√ß√£o:
```bash
# Testar conectividade
docker compose exec superset ping spark-master

# Verificar se Thrift Server est√° rodando
docker compose exec spark-master netstat -tlnp | grep 10000

# Usar URI alternativa
# hive://localhost:10000/default (se acessando externamente)
```

### Q: Drivers Trino/Hive n√£o encontrados
**A:** Instale depend√™ncias no container:
```bash
# Entrar no container Superset
docker compose exec superset bash

# Instalar drivers
pip install pyhive thrift thrift-sasl sqlalchemy-trino

# Reiniciar Superset
docker compose restart superset
```

### Q: Dashboards n√£o carregam dados
**A:** Verifique conex√£o e permiss√µes:
```bash
# Testar conex√£o SQL
docker compose exec superset superset db upgrade

# Verificar datasets (comando n√£o existe no Superset)
# Use a interface web em http://localhost:8088 > Data > Datasets

# Testar query manual no SQL Lab
```

---

## üîß Performance e Recursos

![Performance Tuning](https://media.giphy.com/media/3o7abB06u9bNzA8lu8/giphy.gif)

### Q: Sistema muito lento
**A:** Otimize configura√ß√µes de recursos:
```bash
# Verificar uso atual
docker stats

# Aumentar recursos Docker Desktop
# Settings > Resources > Memory: 8GB+, CPUs: 4+

# Otimizar configura√ß√µes Spark
SPARK_DRIVER_MEMORY=512m
SPARK_EXECUTOR_MEMORY=512m
SPARK_DRIVER_CORES=1
SPARK_EXECUTOR_CORES=1
```

### Q: MinIO com alta lat√™ncia
**A:** Verifique configura√ß√µes de rede e disco:
```bash
# Testar velocidade de disco
docker compose exec minio dd if=/dev/zero of=/data/test bs=1M count=100

# Verificar logs MinIO
docker compose logs minio | grep -i error

# Otimizar configura√ß√µes
# Usar SSD se poss√≠vel
# Aumentar MINIO_BROWSER_REDIRECT_URL se necess√°rio
```

### Q: Kafka com lag alto
**A:** Ajuste configura√ß√µes de throughput:
```bash
# Verificar t√≥picos
docker compose exec kafka kafka-topics --bootstrap-server kafka:29092 --list

# Verificar consumer lag
docker compose exec kafka kafka-consumer-groups --bootstrap-server kafka:29092 --describe --all-groups

# Otimizar configura√ß√µes no [`docker-compose.yml`](../docker-compose.yml)
# (Nota: estas vari√°veis n√£o est√£o configuradas no projeto atual)
# Para ajustar parti√ß√µes, use comandos kafka-topics diretamente
```

---

## üìÅ Dados e Pipeline



### Q: Pipeline falha na camada Bronze
**A:** Verifique dados de entrada e schema:
```bash
# Verificar dados na Landing
docker compose exec minio mc ls local/datalake/landing/

# Verificar logs do job
docker compose logs spark-master | grep -i bronze

# Testar leitura manual
docker compose exec spark-master pyspark
# >>> df = spark.read.json("s3a://datalake/landing/")
# >>> df.show()
```

### Q: An√°lise de sentimentos falha
**A:** Verifique modelos NLP e depend√™ncias:
```bash
# O projeto usa an√°lise de sentimento baseada em dicion√°rio, n√£o Hugging Face
# Verificar se palavras-chave est√£o carregadas corretamente
docker compose exec spark-master python -c "
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
print('Spark session criada com sucesso')
"

# Verificar logs detalhados
docker compose logs spark-master | grep -i nlp
```

### Q: Dados n√£o aparecem no Trino
**A:** Verifique cat√°logo e metastore:
```bash
# Conectar ao Trino CLI
docker compose exec trino-coordinator trino --server localhost:8080 --catalog delta --schema default

# Listar cat√°logos
SHOW CATALOGS;

# Listar schemas
SHOW SCHEMAS FROM delta;

# Verificar tabelas
SHOW TABLES FROM delta.default;
```

### Q: Controle incremental n√£o funciona
**A:** Verifique arquivo de controle:
```bash
# Verificar arquivo de controle
docker compose exec airflow-webserver cat /opt/airflow/raw_data/controle_progresso_bairros.json

# Verificar permiss√µes
docker compose exec airflow-webserver ls -la /opt/airflow/raw_data/

# Resetar controle se necess√°rio
docker compose exec airflow-webserver rm /opt/airflow/raw_data/controle_progresso_bairros.json
```

---

## üÜò Comandos de Emerg√™ncia

![Emergency Commands](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExM3lqZzdpbDZkczF3MWw3emtrcDFpamw2MDh2ZDdjNWVnYXB1dGJwdCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/phJ6eMRFYI6CQ/giphy.gif)

### Reset Completo do Sistema
```bash
# CUIDADO: Remove todos os dados
docker compose down -v
docker system prune -f
docker volume prune -f
[`./scripts/start.sh`](../scripts/start.sh) --clean-volumes
```

### Backup de Dados Cr√≠ticos
```bash
# Backup configura√ß√µes Airflow
docker compose exec airflow-webserver tar -czf /tmp/airflow_backup.tar.gz /opt/airflow/dags /opt/airflow/scripts

# Backup dados MinIO
docker compose exec minio mc mirror local/datalake /tmp/minio_backup/

# Copiar backups para host
docker cp $(docker compose ps -q airflow-webserver):/tmp/airflow_backup.tar.gz ./backups/
```

### Logs de Debugging Completo
```bash
# Coletar todos os logs
mkdir -p debug_logs
docker compose logs > debug_logs/all_services.log
docker compose ps > debug_logs/containers_status.txt
docker stats --no-stream > debug_logs/resources_usage.txt
docker system df > debug_logs/disk_usage.txt
```

### Teste de Conectividade Completo
```bash
#!/bin/bash
# Script de teste completo
echo "=== TESTE DE CONECTIVIDADE ==="

services=("airflow-webserver:8089" "superset:8088" "minio:9001" "trino-coordinator:8080" "spark-master:8082" "spark-worker:8081")

for service in "${services[@]}"; do
    IFS=':' read -r name port <<< "$service"
    if curl -f -s "http://localhost:$port" > /dev/null; then
        echo "‚úÖ $name ($port): OK"
    else
        echo "‚ùå $name ($port): FALHA"
    fi
done
```

---



