# üîÑ Pipeline e DAGs



![Pipeline Flow](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExcnRrNW1xaGt4d3ZseDNzZjl3bGlmYjF3c2VseHBjMGVoZG4zZTdnciZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/fAeh5MTn54HFiyColC/giphy.gif)

Este documento detalha a orquestra√ß√£o completa do pipeline de dados, desde a coleta at√© a an√°lise de sentimentos, usando Apache Airflow como motor de orquestra√ß√£o.

---

## üìã √çndice

- [üéØ Vis√£o Geral do Pipeline](#-vis√£o-geral-do-pipeline)
- [üîß DAGs Implementadas](#-dags-implementadas)
- [‚ö° Jobs Spark](#-jobs-spark)
- [üîß Troubleshooting](#-troubleshooting)
- [üîÑ Fluxo de Execu√ß√£o](#-fluxo-de-execu√ß√£o)
- [üìä Monitoramento e Alertas](#-monitoramento-e-alertas)

---

##  Vis√£o Geral do Pipeline



### Arquitetura de Orquestra√ß√£o

```mermaid
graph TD
    A[Airflow Scheduler] --> B[DAG Coleta Google Maps]
    A --> C[DAG Pipeline Principal]
    A --> D[DAG Monitoramento]
    
    B --> E[Mock Google Maps API]
    B --> F[Apache Kafka]
    F --> G[MinIO Landing]
    
    C --> H[Spark Job: Landing‚ÜíBronze]
    C --> I[Spark Job: Bronze‚ÜíSilver]
    C --> J[Spark Job: Silver‚ÜíGold]
    
    H --> K[Delta Lake Bronze]
    I --> L[Delta Lake Silver]
    J --> M[Delta Lake Gold]
    
    D --> N[Health Checks]
    D --> O[Quality Metrics]
    D --> P[Alertas Email]
```

### Camadas do Data Lakehouse

| Camada | Formato | Descri√ß√£o | Reten√ß√£o |
|--------|---------|-----------|----------|
| **Landing** | JSON | Dados brutos da API | 30 dias |
| **Bronze** | Delta Lake | Dados estruturados | 90 dias |
| **Silver** | Delta Lake | Dados enriquecidos com NLP | 2 anos |
| **Gold** | Delta Lake | KPIs e m√©tricas agregadas | 5 anos |

---

## üìÖ DAGs Implementados

![Airflow DAGs](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExdGY2bXNsM2RjZmE2bjJ6ZWoyNWl5ZXM5dHN0azVreWhmY2o4ZHZweSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/WSqsdbIH6mLrHe78tJ/giphy.gif)


### 1. DAG Coleta Google Maps ([`dag_coleta_google_maps.py`](../mnt/airflow/dags/dag_coleta_google_maps.py))

**Objetivo:** Coletar avalia√ß√µes de ag√™ncias Santander via Google Maps API

**Schedule:** Manual (on-demand)

**Tasks:**
```python
verificar_quota_google_maps >> obter_bairros_pendentes >> coletar_dados_google_maps >> 
salvar_dados >> verificar_qualidade_dados >> verificar_integridade_completa >> 
gerar_relatorio_final
```

**Configura√ß√µes principais:**
```python
default_args = {
    'owner': 'engenharia-dados',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False
}
```

### 2. DAG Pipeline Principal ([`datamaster_pipeline_dag_fixed.py`](../mnt/airflow/dags/datamaster_pipeline_dag_fixed.py))

**Objetivo:** Processar dados atrav√©s das camadas medalh√£o

**Schedule:** Manual (on-demand)

**Tasks:**
```python
start_pipeline >> preprocessing_group >> main_processing_group >> 
post_processing_group >> end_pipeline
```

**BashOperators (Execu√ß√£o Direta no Container Spark):**
```python
landing_to_bronze = BashOperator(
    task_id='landing_to_bronze',
    bash_command='''
    docker exec spark-master /opt/spark/bin/spark-submit \
        --packages io.delta:delta-core_2.12:2.4.0 \
        --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
        --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
        --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
        --conf "spark.hadoop.fs.s3a.access.key=minio" \
        --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
        /opt/bitnami/spark/jobs/landing_to_bronze.py
    '''
)
```

### 3. DAG Limpeza ([`cleanup_datalake_dag.py`](../mnt/airflow/dags/cleanup_datalake_dag.py))

**Objetivo:** Limpeza e manuten√ß√£o do Data Lakehouse

**Schedule:** Semanal

**Tasks:**
- `cleanup_landing_layer`
- `cleanup_bronze_layer` 
- `optimize_delta_tables`
- `vacuum_old_files`

---

## ‚öôÔ∏è Configura√ß√µes dos Jobs

![Job Configuration](https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExcXVqd2F1ZDMzN3ZmZjVhbng1eWJmZHFqYTVjZzdpaTh1bDJyaHozNiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/vpURqIvpuDguQ/giphy.gif)

### Configura√ß√µes Principais

```python
default_args = {
    'owner': 'engenharia-dados',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False
}
```

---

## ‚ö° Jobs Spark



### 1. Landing to Bronze ([`landing_to_bronze.py`](../mnt/spark/jobs/landing_to_bronze.py))

**Fun√ß√£o:** Estruturar dados JSON brutos em formato Delta Lake

```python
def ingest_landing_to_bronze(spark, landing_path, bronze_path):
    """Ingere dados da landing para Bronze com processamento completo"""
    
    # Configurar Spark para Delta Lake
    spark.conf.set("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    spark.conf.set("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    
    # Ler dados JSON da Landing
    df_landing = spark.read.option("multiline", "true").json(landing_path)
    
    # Aplicar transforma√ß√µes e schema
    df_bronze = df_landing.select(
        col("place_id").alias("agencia_id"),
        col("name").alias("nome_agencia"), 
        col("rating").cast("double").alias("rating_agencia"),
        col("total_user_ratings_count").alias("total_avaliacoes"),
        explode(col("reviews")).alias("review_data")
    )
    
    # Salvar em formato Delta Lake
    df_bronze.write.format("delta").mode("overwrite").save(bronze_path)
```

### 2. Bronze to Silver ([`bronze_to_silver_fixed.py`](../mnt/spark/jobs/bronze_to_silver_fixed.py))

**Fun√ß√£o:** Enriquecer dados com an√°lise de sentimentos e detec√ß√£o de PII

```python
def sentiment_analysis_ptbr_udf():
    """UDF para an√°lise de sentimento em portugu√™s brasileiro"""
    
    def analyze_sentiment(text):
        if text is None or text.strip() == "":
            return "neutro"
        
        text_lower = text.lower()
        
        positivas_count = len([palavra for palavra in palavras_positivas if palavra in text_lower])
        negativas_count = len([palavra for palavra in palavras_negativas if palavra in text_lower])
        
        if positivas_count > negativas_count:
            return "positivo"
        elif negativas_count > positivas_count:
            return "negativo"
        else:
            return "neutro"
    
    return udf(analyze_sentiment, StringType())

def detect_pii_udf():
    """UDF para detec√ß√£o de PII usando regex"""
    
    def detect_pii(text):
        if text is None or text.strip() == "":
            return (False, [], text)
        
        pii_detected = []
        anonymized_text = text
        
        # CPF, Email, Telefone patterns
        if re.search(r'\b\d{3}\.?\d{3}\.?\d{3}-?\d{2}\b', text):
            pii_detected.append("cpf")
            anonymized_text = re.sub(r'\b\d{3}\.?\d{3}\.?\d{3}-?\d{2}\b', '[CPF_ANONIMIZADO]', anonymized_text)
        
        return (len(pii_detected) > 0, pii_detected, anonymized_text)
    
    return udf(detect_pii, StructType([
        StructField("tem_pii", BooleanType(), True),
        StructField("tipos_pii", ArrayType(StringType()), True),
        StructField("texto_anonimizado", StringType(), True)
    ]))
```

### 3. Silver to Gold ([`silver_to_gold_working.py`](../mnt/spark/jobs/silver_to_gold_working.py))

**Fun√ß√£o:** Gerar KPIs e m√©tricas de neg√≥cio

```python
def gerar_kpis_negocio(df_silver):
    # KPIs por ag√™ncia
    kpis_agencia = df_silver.groupBy("agencia_id", "nome_agencia").agg(
        avg("rating").alias("rating_medio"),
        count("*").alias("total_avaliacoes"),
        sum(when(col("sentimento") == "POSITIVE", 1).otherwise(0)).alias("avaliacoes_positivas"),
        sum(when(col("risco_reputacional") == "ALTO", 1).otherwise(0)).alias("riscos_altos")
    )
    
    # Tend√™ncias temporais
    tendencias = df_silver.groupBy(
        date_format("data_avaliacao", "yyyy-MM").alias("mes_ano")
    ).agg(
        avg("rating").alias("rating_medio_mensal"),
        avg("sentimento_score").alias("sentimento_medio_mensal")
    )
    
    return kpis_agencia, tendencias
```

---

## üîß Troubleshooting



### Dicas de Solu√ß√£o de Problemas

1. **Verificar logs**: Airflow, Spark, MinIO
2. **Testar DAGs**: Manualmente, via Airflow UI
3. **Revisar configura√ß√µes**: DAGs, Jobs Spark, Airflow

---

## üîÑ Fluxo de Execu√ß√£o



### Sequ√™ncia T√≠pica de Execu√ß√£o

1. **Trigger Manual ou Autom√°tico**
   ```bash
   # Via Airflow UI ou API
   curl -X POST "http://localhost:8089/api/v1/dags/dag_coleta_google_maps/dagRuns"
   ```

2. **Coleta de Dados**
   - Verifica√ß√£o de quota API
   - Sele√ß√£o de bairros pendentes
   - Coleta via Google Maps API
   - Envio para Kafka
   - Persist√™ncia em MinIO Landing

3. **Processamento Spark**
   - Landing ‚Üí Bronze: Estrutura√ß√£o
   - Bronze ‚Üí Silver: Enriquecimento NLP
   - Silver ‚Üí Gold: Agrega√ß√µes

4. **Valida√ß√£o e Qualidade**
   - Verifica√ß√£o de integridade
   - M√©tricas de qualidade
   - Alertas autom√°ticos

### Depend√™ncias entre DAGs

```mermaid
graph LR
    A[DAG Coleta] --> B[DAG Pipeline Principal]
    B --> C[DAG Limpeza]
    C --> D[Manuten√ß√£o/Otimiza√ß√£o]
```

---

## üìä Monitoramento e Alertas



### M√©tricas Coletadas

| M√©trica | Descri√ß√£o | Threshold |
|---------|-----------|-----------|
| **Taxa de Sucesso** | % de tasks bem-sucedidas | > 95% |
| **Lat√™ncia M√©dia** | Tempo m√©dio de execu√ß√£o | < 30 min |
| **Qualidade de Dados** | % registros v√°lidos | > 98% |
| **Uso de Recursos** | CPU/RAM Spark | < 80% |

### Alertas Configurados

```python
def verificar_anomalias():
    # Verificar falhas recorrentes
    falhas = get_dag_run_failures(last_n_days=1)
    if falhas > 3:
        send_alert("M√∫ltiplas falhas detectadas")
    
    # Verificar qualidade dos dados
    qualidade = calculate_data_quality()
    if qualidade < 0.95:
        send_alert(f"Qualidade baixa: {qualidade:.2%}")
    
    # Verificar lat√™ncia
    latencia = get_average_task_duration()
    if latencia > timedelta(minutes=45):
        send_alert(f"Alta lat√™ncia: {latencia}")
```

### Dashboard de Monitoramento

**M√©tricas em Tempo Real:**
- Status das DAGs
- Execu√ß√µes recentes
- Qualidade dos dados
- Performance do cluster Spark

**Acesso:** http://localhost:8089 (Airflow UI)

---

## üìñ Di√°rio de Bordo

### Decis√µes Arquiteturais

**Por que Airflow LocalExecutor?**
- Simplicidade para ambiente local
- Menor overhead que CeleryExecutor
- Adequado para volume de dados atual

**Por que BashOperator com docker exec?**
- Execu√ß√£o direta no container Spark
- Evita problemas de conectividade Airflow-Spark
- Controle total sobre configura√ß√µes Spark

**Por que controle incremental por arquivo?**
- Resili√™ncia a falhas
- Facilita reprocessamento
- Auditoria completa

### Otimiza√ß√µes Implementadas

1. **Retry com Backoff Exponencial**
2. **Health Checks Autom√°ticos**
3. **Particionamento Inteligente**
4. **Cache de Modelos NLP**
5. **Compress√£o Delta Lake**

### Li√ß√µes Aprendidas

- **Sempre validar conectividade** entre containers
- **Configurar timeouts generosos** para primeira execu√ß√£o
- **Implementar idempot√™ncia** em todos os jobs
- **Monitorar recursos Spark** continuamente

---

