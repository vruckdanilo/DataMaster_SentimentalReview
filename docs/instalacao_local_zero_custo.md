# üè† Instala√ß√£o Local Zero Custo

![Local Installation](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExbDV6dGNicnhvMmI2dzVhMDQ5Nnd5ZGR5OHA0YjJubmp4cDR5bHJlbiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/lYZjoIy0UOEJa/giphy.gif) - DataMaster SentimentalReview


Este guia te leva do zero ao ambiente completo funcionando em menos de 10 minutos. Eu otimizei tudo para rodar no seu Docker Desktop sem precisar de cloud, cr√©ditos ou cart√£o de cr√©dito.

---

## üìã √çndice

- [üéØ Pr√©-requisitos](#-pr√©-requisitos)
- [‚öôÔ∏è Configura√ß√£o do Ambiente](#Ô∏è-configura√ß√£o-do-ambiente)
- [üöÄ Instala√ß√£o Autom√°tica](#-instala√ß√£o-autom√°tica)
- [‚úÖ Valida√ß√£o e Smoke Tests](#-valida√ß√£o-e-smoke-tests)
- [üîß Troubleshooting](#-troubleshooting)
- [üìñ Di√°rio de Bordo](#-Di√°rio-de-Bordo)

---

## üìã Pr√©-requisitos


### Sistema Operacional
- **Linux:** Ubuntu 20.04+, CentOS 8+, ou similar
- **macOS:** 10.15+ (Catalina ou superior)
- **Windows:** Windows 10/11 com WSL2

### Hardware M√≠nimo
| Componente | M√≠nimo | Recomendado | Observa√ß√µes |
|------------|--------|-------------|-------------|
| **RAM** | 8GB | 16GB | Spark √© memory-intensive |
| **CPU** | 4 cores | 8 cores | Processamento paralelo |
| **Disco** | 20GB livre | 50GB livre | Imagens Docker + dados |


### Software Obrigat√≥rio

**1. Docker Desktop**
```bash
# Ubuntu/Debian
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER

# macOS
brew install --cask docker

# Windows
# Baixar do site oficial: https://docker.com/products/docker-desktop
```

**2. Docker Compose V2**
```bash
# Verificar se j√° est√° instalado
docker compose version

# Se n√£o estiver, instalar
sudo apt-get install docker-compose-plugin  # Ubuntu
brew install docker-compose                 # macOS
```

**3. Git**
```bash
sudo apt-get install git  # Ubuntu
brew install git          # macOS
```

### üîç Verifica√ß√£o da Instala√ß√£o

Pr√©-requisitos
```bash
# Verificar Docker
docker --version
docker compose version

# Verificar recursos
free -h                    # RAM dispon√≠vel
df -h                      # Espa√ßo em disco
nproc                      # N√∫mero de CPUs

# Testar Docker
docker run hello-world
```

---

## ‚öôÔ∏è Configura√ß√£o do Ambiente



### 1. Clone do Reposit√≥rio
```bash
git clone <repository-url>
cd DataMaster_SentimentalReview
```

### 2. Configura√ß√£o do .env

O projeto j√° vem com um [`.env`](../.env) pr√©-configurado, mas voc√™ pode personalizar:

```bash
# O arquivo .env j√° est√° configurado e pronto para uso
# Voc√™ pode edit√°-lo se necess√°rio:
nano [`.env`](../.env)
```

**Vari√°veis principais do [`.env`](../.env):**

```bash
# ====================================================================
# üîë APIS EXTERNAS (OPCIONAIS PARA DEMO)
# ====================================================================


# ====================================================================
# üóÑÔ∏è CONFIGURA√á√ïES DE BANCO
# ====================================================================

POSTGRES_DB=airflow_db
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow

# ====================================================================
# üíæ MINIO (S3 LOCAL)
# ====================================================================

MINIO_ACCESS_KEY=minio
MINIO_SECRET_KEY=minio123
MINIO_ENDPOINT=minio:9000

# ====================================================================
# ‚ö° SPARK (OTIMIZADO PARA LOCAL)
# ====================================================================

SPARK_DRIVER_MEMORY=512m
SPARK_EXECUTOR_MEMORY=512m
SPARK_DRIVER_CORES=1
SPARK_EXECUTOR_CORES=1

# ====================================================================
# üîÑ AIRFLOW
# ====================================================================

AIRFLOW_ADMIN_USERNAME=admin
AIRFLOW_ADMIN_PASSWORD=admin
```

### 3. Configura√ß√£o de Recursos Docker

**Docker Desktop Settings:**
- **Memory:** 6-8GB (m√≠nimo 4GB)
- **CPUs:** 4-6 cores
- **Disk:** 50GB+ espa√ßo livre

**Linux - Configurar limites:**
```bash
# Editar daemon.json
sudo nano /etc/docker/daemon.json

{
  "default-runtime": "runc",
  "storage-driver": "overlay2",
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}

# Reiniciar Docker
sudo systemctl restart docker
```

---

## üöÄ Inicializa√ß√£o do Sistema


![Automated Installation](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZWNzamFqMGYyOW84ZmYxZjZnbXVra2dxcThpbHAwMWhkeHBmbDY3bCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/l1RCv0zXG5PhNRFEWl/giphy.gif)

### M√©todo 1: Script Autom√°tico (Recomendado)

```bash
# Executar script principal
[`./scripts/start.sh`](../scripts/start.sh)

# O script faz TUDO automaticamente:
# ‚úÖ Verifica pr√©-requisitos
# ‚úÖ Limpa ambiente anterior
# ‚úÖ Constr√≥i imagens Docker (5-10 min)
# ‚úÖ Inicia servi√ßos principais
# ‚úÖ Configura buckets MinIO
# ‚úÖ Cria t√≥picos Kafka
# ‚úÖ Configura conex√µes Airflow
# ‚úÖ Executa health checks
# ‚úÖ Mostra URLs de acesso
```

**Op√ß√µes do script:**
```bash
# Instala√ß√£o limpa (remove dados anteriores)
[`./scripts/start.sh`](../scripts/start.sh) --clean-volumes

# Pular testes autom√°ticos
[`./scripts/start.sh`](../scripts/start.sh) --no-tests

# N√£o reconstruir imagens
[`./scripts/start.sh`](../scripts/start.sh) --no-build

# Recriar apenas Superset
[`./scripts/start.sh`](../scripts/start.sh) --recreate-superset

# Ver todas as op√ß√µes
[`./scripts/start.sh`](../scripts/start.sh) --help
```

### M√©todo 2: Manual (Para Debugging)

```bash
# 1. Construir imagens base
docker compose --profile build build spark-base

# 2. Construir demais servi√ßos
docker compose build

# 3. Iniciar servi√ßos
docker compose up -d

# 4. Aguardar inicializa√ß√£o (5-10 min)
docker compose ps

# 5. Configurar MinIO
docker compose exec minio mc alias set local http://localhost:9000 minio minio123
docker compose exec minio mc mb local/datalake

# 6. Criar usu√°rio admin
docker compose exec airflow-webserver airflow users create \
  --username admin --password admin --firstname Admin \
  --lastname User --role Admin --email admin@example.com
```

### Acompanhamento da Instala√ß√£o

**Logs em tempo real:**
```bash
# Todos os servi√ßos
docker compose logs -f

# Servi√ßo espec√≠fico
docker compose logs -f airflow-webserver
docker compose logs -f spark-master
docker compose logs -f superset
```

**Status dos containers:**
```bash
# Verificar status
docker compose ps

# Verificar sa√∫de
docker compose ps --format "table {{.Name}}\t{{.Status}}\t{{.Ports}}"
```

---

## ‚úÖ Valida√ß√£o e Smoke Tests



### Teste Autom√°tico (via script)
```bash
# Executar testes integrados
[`./scripts/start.sh`](../scripts/start.sh) --no-build  # Pula rebuild, s√≥ testa
```

### Testes Manuais

**1. Testar Endpoints Principais**
```bash
# Airflow
curl -f http://localhost:8089/health
echo "Airflow: $?"

# Superset  
curl -f http://localhost:8088/health
echo "Superset: $?"

# MinIO
curl -f http://localhost:9000/minio/health/live
echo "MinIO: $?"

# Trino
curl -f http://localhost:8080/v1/info
echo "Trino: $?"

# Spark Master
curl -f http://localhost:8082
echo "Spark: $?"

# Mock API
curl -f http://localhost:3001/textsearch
echo "Mock API: $?"
```

**2. Testar Conectividade Interna**
```bash
# Kafka
docker compose exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# PostgreSQL
docker compose exec postgres-airflow pg_isready -U airflow

# MySQL
docker compose exec mysql mysqladmin ping -h localhost
```

**3. Testar Integra√ß√£o Spark + MinIO**
```bash
# Executar job de teste
docker compose exec spark-master spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://spark-master:7077 \
  /opt/bitnami/spark/examples/jars/spark-examples_2.12-3.5.0.jar 10
```

### Checklist de Valida√ß√£o

| Servi√ßo | URL | Status Esperado | Credenciais |
|---------|-----|-----------------|-------------|
| ‚úÖ Airflow | http://localhost:8089 | Login page | admin/admin |
| ‚úÖ Superset | http://localhost:8088 | Login page | admin/admin123 |
| ‚úÖ Trino | http://localhost:8080 | Query interface | - |
| ‚úÖ Spark Master | http://localhost:8082 | Cluster info | - |
| ‚úÖ Spark Worker | http://localhost:8081 | Worker info | - |
| ‚úÖ MinIO Console | http://localhost:9001 | Login page | minio/minio123 |
| ‚úÖ Mock API | http://localhost:3001 | JSON response | - |

---

## üîß Troubleshooting

![Troubleshooting](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExeTVtbGowdjRvM2VsbHB1eXR4OXc0NzQwNTU5YnJleHE4azl0enZuayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/PnpkimJ5mrZRe/giphy.gif)

### Problemas Comuns

**1. Erro: "Port already in use"**
```bash
# Verificar processos usando as portas
sudo lsof -i :8089  # Airflow
sudo lsof -i :8088  # Superset
sudo lsof -i :8080  # Trino

# Parar processos conflitantes
sudo systemctl stop apache2    # Se houver Apache
sudo systemctl stop nginx      # Se houver Nginx

# Ou mudar portas no [`docker-compose.yml`](../docker-compose.yml)
```

**2. Erro: "Cannot connect to Docker daemon"**
```bash
# Verificar se Docker est√° rodando
sudo systemctl status docker

# Iniciar Docker
sudo systemctl start docker

# Adicionar usu√°rio ao grupo docker
sudo usermod -aG docker $USER
newgrp docker  # Ou fazer logout/login
```

**3. Erro: "Out of memory" ou containers crashando**
```bash
# Verificar uso de mem√≥ria
docker stats

# Aumentar mem√≥ria no Docker Desktop
# Settings > Resources > Memory > 8GB+

# Ou reduzir configura√ß√µes no [`.env`](../.env)
SPARK_DRIVER_MEMORY=256m
SPARK_EXECUTOR_MEMORY=256m
```

**4. Erro: "No space left on device"**
```bash
# Limpar imagens n√£o utilizadas
docker system prune -f

# Limpar volumes √≥rf√£os
docker volume prune -f

# Verificar espa√ßo
df -h
docker system df
```

**5. Containers ficam "unhealthy"**
```bash
# Verificar logs espec√≠ficos
docker compose logs [service-name]

# Reiniciar servi√ßo espec√≠fico
docker compose restart [service-name]

# Recriar container
docker compose up -d --force-recreate [service-name]
```

### Debugging Avan√ßado

**Logs detalhados:**
```bash
# Airflow scheduler
docker compose logs -f airflow-scheduler | grep ERROR

# Spark jobs
docker compose logs -f spark-master | grep -i error

# Superset inicializa√ß√£o
docker compose logs -f superset | tail -100
```

**Acesso aos containers:**
```bash
# Entrar no container Airflow
docker compose exec airflow-webserver bash

# Entrar no container Spark
docker compose exec spark-master bash

# Verificar configura√ß√µes
docker compose exec airflow-webserver env | grep AIRFLOW
```

**Testes de conectividade:**
```bash
# Testar conectividade entre containers
docker compose exec airflow-webserver ping minio
docker compose exec trino ping postgres-airflow
docker compose exec superset ping trino-coordinator
```

### Solu√ß√µes para Problemas Espec√≠ficos

**Airflow n√£o carrega DAGs:**
```bash
# Verificar diret√≥rio de DAGs
docker compose exec airflow-webserver ls -la /opt/airflow/dags/

# Verificar logs do scheduler
docker compose logs airflow-scheduler | grep -i dag

# For√ßar refresh
docker compose exec airflow-webserver airflow dags list-import-errors
```

**Spark jobs falham:**
```bash
# Verificar worker conectado
docker compose exec spark-master curl http://localhost:8082/json/

# Testar job simples
docker compose exec spark-master spark-submit --version

# Verificar configura√ß√µes MinIO/S3
docker compose exec spark-master env | grep MINIO
```

**Superset n√£o conecta ao Trino:**
```bash
# Testar conectividade
docker compose exec superset ping trino

# Verificar drivers instalados
docker compose exec superset pip list | grep trino

# Testar conex√£o manual
docker compose exec superset python -c "
import trino
conn = trino.dbapi.connect(host='trino', port=8080, user='admin')
print('Conex√£o OK')
"
```

---

## üéØ Primeiros Passos

![Getting Started](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExY3Y1N3hsZTdqM3l5cG41MTB2dm15ZnNzODg1ZmsyeThoNHlzd3Z6MiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/HNtm9TDgxGza8/giphy.gif)

### Ap√≥s Instala√ß√£o Bem-sucedida

**1. Explorar Interfaces**
- Fa√ßa login no Airflow (admin/admin)
- Explore a interface do Superset
- Navegue pelo Spark Master UI
- Acesse o MinIO Console

**2. Executar Pipeline de Exemplo**
```bash
# Via Airflow UI
# 1. Acesse http://localhost:8089
# 2. Ative a DAG "datamaster_pipeline_dag_fixed"
# 3. Execute manualmente
# 4. Acompanhe execu√ß√£o

# Via linha de comando
docker compose exec airflow-webserver airflow dags trigger datamaster_pipeline_dag_fixed
```

**3. Explorar Dados**
```bash
# Verificar buckets MinIO
docker compose exec minio mc ls local/datalake/

# Consultar via Trino
docker compose exec trino-coordinator trino --server localhost:8080 --catalog iceberg --schema default
```

**4. Configurar Dashboards**
- Configure conex√£o Superset ‚Üí Trino
- Crie datasets baseados nas tabelas Gold
- Desenvolva dashboards de sentimento

### Personaliza√ß√£o

**APIs Reais:**
1. Obtenha chave Google Maps API
2. Configure OpenAI API key
3. Descomente vari√°veis no [`.env`](../.env)
4. Reinicie containers

**Configura√ß√µes de Produ√ß√£o:**
1. Aumente recursos Spark
2. Configure SSL/TLS
3. Implemente backup autom√°tico
4. Configure alertas por email

### Monitoramento Cont√≠nuo

```bash
# Script de monitoramento
cat > monitor.sh << 'EOF'
#!/bin/bash
while true; do
  echo "=== $(date) ==="
  docker compose ps --format "table {{.Name}}\t{{.Status}}"
  echo ""
  sleep 60
done
EOF

chmod +x monitor.sh
./monitor.sh
```

---

## üìñ Di√°rio de Bordo

### Atalhos e Truques que uso no dia a dia

**1. Aliases √∫teis:**
```bash
# Adicionar ao ~/.bashrc
alias dps='docker compose ps'
alias dlogs='docker compose logs -f'
alias dexec='docker compose exec'
alias ddown='docker compose down'
alias dup='docker compose up -d'
```

**2. Scripts de desenvolvimento:**
```bash
# Restart r√°pido do Airflow
restart_airflow() {
  docker compose restart airflow-webserver airflow-scheduler
}

# Limpar logs antigos
clean_logs() {
  docker compose exec airflow-webserver find /opt/airflow/logs -name "*.log" -mtime +7 -delete
}
```

**3. Configura√ß√µes otimizadas:**
- Uso health checks para evitar race conditions
- Configurei timeouts generosos para primeira inicializa√ß√£o
- Implementei retry autom√°tico com backoff exponencial
- Separei configura√ß√µes por ambiente ([`.env`](../.env))

**4. Debugging eficiente:**
- Sempre verifico logs antes de reiniciar containers
- Uso `docker stats` para monitorar recursos
- Mantenho scripts de teste para valida√ß√£o r√°pida
- Documento todos os problemas e solu√ß√µes

### Resultados

- ‚úÖ **Automa√ß√£o completa:** Um script resolve tudo
- ‚úÖ **Configura√ß√£o robusta:** Health checks e valida√ß√µes
- ‚úÖ **Troubleshooting detalhado:** Solu√ß√µes para problemas reais
- ‚úÖ **Otimiza√ß√£o de recursos:** Funciona em hardware limitado
- ‚úÖ **Documenta√ß√£o pr√°tica:** Baseada em experi√™ncia real

---

## üìä M√≠dia Sugerida para este Doc

| Arquivo | Descri√ß√£o | Momento de Uso |
|---------|-----------|----------------|
| `docs/media/docker-startup.gif` | Containers subindo em sequ√™ncia | In√≠cio do documento |
| `docs/media/health-checks.gif` | Valida√ß√£o autom√°tica de servi√ßos | Se√ß√£o de valida√ß√£o |
| `docs/media/troubleshooting-flow.gif` | Fluxo de debugging | Se√ß√£o troubleshooting |
| `docs/media/first-login.gif` | Primeiro acesso √†s interfaces | Pr√≥ximos passos |

---

**Dica R√°pida:** Se algo der errado, sempre comece com `docker compose logs [service]`. 90% dos problemas ficam claros nos logs.

**Erro que j√° cometi aqui:** Tentar acessar servi√ßos antes dos health checks passarem. Paci√™ncia √© fundamental na primeira inicializa√ß√£o!

---
