#!/usr/bin/env python3


from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta import configure_spark_with_delta_pip
import logging
from datetime import datetime

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_spark_session():
    """Cria sess√£o Spark otimizada para Delta Lake"""
    
    builder = SparkSession.builder \
        .appName("DataMaster_SilverToGold_Working") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minio") \
        .config("spark.hadoop.fs.s3a.secret.key", "minio123") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    
    return configure_spark_with_delta_pip(builder).getOrCreate()

def main():
    """Fun√ß√£o principal que executa o pipeline Silver ‚Üí Gold"""
    
    logger.info("=== INICIANDO PIPELINE SILVER ‚Üí GOLD (PRODU√á√ÉO) ===")
    
    try:
        # Criar sess√£o Spark
        spark = create_spark_session()
        
        # Paths dos dados
        silver_path = "s3a://datalake/silver/google_maps_reviews_enriched"
        gold_base_path = "s3a://datalake/gold"
        
        logger.info(f"üìñ Lendo dados Silver de: {silver_path}")
        
        # Ler dados da camada Silver
        df_silver = spark.read.format("delta").load(silver_path)
        
        logger.info(f"‚úÖ Dados Silver carregados: {df_silver.count()} registros")
        
        # Criar database datalake se n√£o existir
        spark.sql("CREATE DATABASE IF NOT EXISTS datalake")
        logger.info("‚úÖ Database 'datalake' criado/verificado")
        
        # 1. Agency Performance KPIs
        logger.info("üè≠ Gerando agency_performance_kpis...")
        df_kpis = df_silver.groupBy("place_id", "nome_agencia", "bairro").agg(
            count("*").alias("total_reviews"),
            avg("rating_review").alias("rating_medio"),
            sum(when(col("sentimento") == "positivo", 1).otherwise(0)).alias("reviews_positivos"),
            sum(when(col("sentimento") == "negativo", 1).otherwise(0)).alias("reviews_negativos"),
            sum(when(col("sentimento") == "neutro", 1).otherwise(0)).alias("reviews_neutros"),
            sum(when(col("tem_pii") == True, 1).otherwise(0)).alias("reviews_com_pii"),
            current_timestamp().alias("gold_timestamp")
        )
        
        kpis_path = f"{gold_base_path}/agency_performance_kpis"
        df_kpis.write.format("delta").mode("overwrite").save(kpis_path)
        
        # Registrar tabela no cat√°logo datalake
        spark.sql(f"DROP TABLE IF EXISTS datalake.agency_performance_kpis")
        spark.sql(f"CREATE TABLE datalake.agency_performance_kpis USING DELTA LOCATION '{kpis_path}'")
        
        logger.info(f"‚úÖ KPIs de ag√™ncia salvos: {df_kpis.count()} registros")
        
        # 2. Temporal Sentiment Analysis
        logger.info("üè≠ Gerando temporal_sentiment_analysis...")
        df_temporal = df_silver.withColumn("mes", month("data_coleta")).groupBy("bairro", "mes").agg(
            count("*").alias("total_reviews"),
            sum(when(col("sentimento") == "positivo", 1).otherwise(0)).alias("positivos"),
            sum(when(col("sentimento") == "negativo", 1).otherwise(0)).alias("negativos"),
            current_timestamp().alias("gold_timestamp")
        )
        
        temporal_path = f"{gold_base_path}/temporal_sentiment_analysis"
        df_temporal.write.format("delta").mode("overwrite").save(temporal_path)
        
        # Registrar tabela no cat√°logo datalake
        spark.sql(f"DROP TABLE IF EXISTS datalake.temporal_sentiment_analysis")
        spark.sql(f"CREATE TABLE datalake.temporal_sentiment_analysis USING DELTA LOCATION '{temporal_path}'")
        
        logger.info(f"‚úÖ An√°lise temporal salva: {df_temporal.count()} registros")
        
        # 3. Risk Alerts (usando reviews negativos como proxy de risco)
        logger.info("üè≠ Gerando risk_alerts...")
        df_alerts = df_silver.filter(col("sentimento") == "negativo").groupBy("place_id", "nome_agencia").agg(
            count("*").alias("total_reviews_negativas"),
            current_timestamp().alias("gold_timestamp")
        )
        
        if df_alerts.count() == 0:
            # Se n√£o h√° alertas, criar registro dummy
            df_alerts = spark.createDataFrame([
                ("sem_alertas", "Nenhum alerta", 0, datetime.now())
            ], ["place_id", "nome_agencia", "total_reviews_negativas", "gold_timestamp"])
        
        alerts_path = f"{gold_base_path}/risk_alerts"
        df_alerts.write.format("delta").mode("overwrite").save(alerts_path)
        
        # Registrar tabela no cat√°logo datalake
        spark.sql(f"DROP TABLE IF EXISTS datalake.risk_alerts")
        spark.sql(f"CREATE TABLE datalake.risk_alerts USING DELTA LOCATION '{alerts_path}'")
        
        logger.info(f"‚úÖ Alertas de risco salvos: {df_alerts.count()} registros")
        
        # 4. Executive Dashboard
        logger.info("üè≠ Gerando executive_dashboard...")
        total_reviews = df_silver.count()
        positivos = df_silver.filter(col("sentimento") == "positivo").count()
        negativos = df_silver.filter(col("sentimento") == "negativo").count()
        total_agencies = df_silver.select("place_id").distinct().count()
        
        df_dashboard = spark.createDataFrame([
            (total_reviews, positivos, negativos, total_agencies, datetime.now())
        ], ["total_reviews", "reviews_positivos", "reviews_negativos", "total_agencias", "gold_timestamp"])
        
        dashboard_path = f"{gold_base_path}/executive_dashboard"
        df_dashboard.write.format("delta").mode("overwrite").save(dashboard_path)
        
        # Registrar tabela no cat√°logo datalake
        spark.sql(f"DROP TABLE IF EXISTS datalake.executive_dashboard")
        spark.sql(f"CREATE TABLE datalake.executive_dashboard USING DELTA LOCATION '{dashboard_path}'")
        
        logger.info(f"‚úÖ Dashboard executivo salvo: {df_dashboard.count()} registros")
        
        # 5. NPS Ranking
        logger.info("üè≠ Gerando nps_ranking...")
        df_nps = df_silver.withColumn("nps_categoria",
                                     when(col("rating_review") >= 4, "Promotor")
                                     .when(col("rating_review") >= 3, "Neutro")
                                     .otherwise("Detrator")) \
            .groupBy("place_id", "nome_agencia").agg(
                count("*").alias("total_avaliacoes"),
                sum(when(col("nps_categoria") == "Promotor", 1).otherwise(0)).alias("promotores"),
                sum(when(col("nps_categoria") == "Detrator", 1).otherwise(0)).alias("detratores"),
                avg("rating_review").alias("rating_medio"),
                current_timestamp().alias("gold_timestamp")
            )
        
        nps_path = f"{gold_base_path}/nps_ranking"
        df_nps.write.format("delta").mode("overwrite").save(nps_path)
        
        # Registrar tabela no cat√°logo datalake
        spark.sql(f"DROP TABLE IF EXISTS datalake.nps_ranking")
        spark.sql(f"CREATE TABLE datalake.nps_ranking USING DELTA LOCATION '{nps_path}'")
        
        logger.info(f"‚úÖ Ranking NPS salvo: {df_nps.count()} registros")
        
        # 6. Business Metrics Summary
        logger.info("üè≠ Gerando business_metrics_summary...")
        df_business = df_silver.groupBy("bairro").agg(
            count("*").alias("total_reviews"),
            avg("rating_review").alias("rating_medio"),
            sum(when(col("sentimento") == "positivo", 1).otherwise(0)).alias("reviews_positivos"),
            current_timestamp().alias("gold_timestamp")
        )
        
        business_path = f"{gold_base_path}/business_metrics_summary"
        df_business.write.format("delta").mode("overwrite").save(business_path)
        
        # Registrar tabela no cat√°logo datalake
        spark.sql(f"DROP TABLE IF EXISTS datalake.business_metrics_summary")
        spark.sql(f"CREATE TABLE datalake.business_metrics_summary USING DELTA LOCATION '{business_path}'")
        
        logger.info(f"‚úÖ Sum√°rio de neg√≥cio salvo: {df_business.count()} registros")
        
        # Estat√≠sticas finais
        logger.info("üìà Estat√≠sticas finais do processamento:")
        logger.info(f"   ‚Ä¢ KPIs de Ag√™ncia: {df_kpis.count()} ag√™ncias analisadas")
        logger.info(f"   ‚Ä¢ An√°lise Temporal: {df_temporal.count()} per√≠odos analisados")
        logger.info(f"   ‚Ä¢ Alertas de Risco: {df_alerts.count()} alertas gerados")
        logger.info(f"   ‚Ä¢ Dashboard Executivo: {df_dashboard.count()} m√©tricas agregadas")
        logger.info(f"   ‚Ä¢ Ranking NPS: {df_nps.count()} ag√™ncias ranqueadas")
        logger.info(f"   ‚Ä¢ Sum√°rio de Neg√≥cio: {df_business.count()} registros de sum√°rio")
        
        # Listar tabelas registradas no cat√°logo datalake
        logger.info("üìã Verificando tabelas registradas no cat√°logo datalake:")
        tables_df = spark.sql("SHOW TABLES IN datalake")
        tables_df.show()
        
        logger.info("‚úÖ Pipeline Silver ‚Üí Gold PRODU√á√ÉO conclu√≠do com sucesso!")
        logger.info("üéØ Todas as 6 tabelas Gold geradas E REGISTRADAS NO CAT√ÅLOGO:")
        logger.info("   ‚úÖ agency_performance_kpis")
        logger.info("   ‚úÖ temporal_sentiment_analysis")
        logger.info("   ‚úÖ risk_alerts")
        logger.info("   ‚úÖ executive_dashboard")
        logger.info("   ‚úÖ nps_ranking")
        logger.info("   ‚úÖ business_metrics_summary")
        logger.info("üìä Tabelas dispon√≠veis para consulta via SQL!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro durante transforma√ß√£o Silver ‚Üí Gold: {e}")
        raise e
    finally:
        spark.stop()

if __name__ == "__main__":
    main()