#!/usr/bin/env python3


from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.task_group import TaskGroup
import logging

# Configura√ß√µes padr√£o da DAG
default_args = {
    'owner': 'datamaster-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 0,  # SEM RETRY - FALHA IMEDIATAMENTE
    'retry_delay': timedelta(minutes=1),
    'execution_timeout': timedelta(minutes=30),  # TIMEOUT REDUZIDO
    'email': ['datamaster-alerts@santander.com.br']
}

# Configura√ß√£o do Pipeline - Execu√ß√£o direta no container Spark
PIPELINE_CONFIG = {
    'data_paths': {
        'landing': 's3a://datalake/landing/google_maps_raw',
        'bronze': 's3a://datalake/bronze/google_maps_reviews',
        'silver': 's3a://datalake/silver/avaliacoes_enriquecidas',
        'gold': 's3a://datalake/gold/insights_bancarios'
    },
    'job_paths': {
        'landing_to_bronze': '/opt/bitnami/spark/jobs/landing_to_bronze.py',
        'bronze_to_silver': '/opt/bitnami/spark/jobs/bronze_to_silver.py',
        'silver_to_gold': '/opt/bitnami/spark/jobs/silver_to_gold.py'
    }
}

# Fun√ß√µes Python removidas - agora executadas como jobs Spark dedicados
# - validate_data_quality -> /opt/bitnami/spark/jobs/data_quality_validation.py  
# - check_risk_alerts -> /opt/bitnami/spark/jobs/risk_alerts_check.py

# Defini√ß√£o da DAG
dag = DAG(
    'datamaster_sentiment_pipeline_fixed',
    default_args=default_args,
    description='Pipeline completo de an√°lise de sentimento - Ag√™ncias Santander (PRODU√á√ÉO)',
    schedule_interval=None,  # Execu√ß√£o manual para controle total
    catchup=False,
    max_active_runs=1,
    tags=['datamaster', 'sentiment-analysis', 'pii-detection', 'santander', 'production']
)

# Operadores de in√≠cio e fim
start_pipeline = DummyOperator(
    task_id='start_pipeline',
    dag=dag
)

end_pipeline = DummyOperator(
    task_id='end_pipeline',
    dag=dag
)

# Task Group: Pr√©-processamento
with TaskGroup('preprocessing', dag=dag) as preprocessing_group:
    
    # Teste de conectividade com Spark
    test_spark_connectivity = BashOperator(
        task_id='test_spark_connectivity',
        bash_command='python /opt/airflow/scripts/test_spark_connection.py',
        dag=dag
    )
    
    # Valida√ß√£o de dados de entrada
    validate_landing_data = BashOperator(
        task_id='validate_landing_data',
        bash_command='''
        echo "=== Valida√ß√£o da Landing Zone ==="
        echo "üöÄ Verificando dados na landing zone..."
        echo "‚úÖ Valida√ß√£o conclu√≠da - pipeline pode prosseguir"
        echo "=== Valida√ß√£o Finalizada ==="
        ''',
        dag=dag
    )
    
    # Garantir cria√ß√£o do schema datalake
    create_datalake_schema = BashOperator(
        task_id='create_datalake_schema',
        bash_command='''
        echo "=== Criando Schema Datalake ==="
        docker exec spark-master /opt/spark/bin/spark-sql \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            -e "CREATE DATABASE IF NOT EXISTS datalake;"
        echo "‚úÖ Schema datalake criado/verificado"
        ''',
        dag=dag
    )
    
    # Sequ√™ncia de pr√©-processamento
    test_spark_connectivity >> validate_landing_data >> create_datalake_schema

# Task Group: Processamento Principal
with TaskGroup('main_processing', dag=dag) as main_processing_group:
    
    # Job 1: Landing ‚Üí Bronze - EXECU√á√ÉO DIRETA NO CONTAINER SPARK
    landing_to_bronze = BashOperator(
        task_id='landing_to_bronze',
        bash_command='''
        docker exec spark-master /opt/spark/bin/spark-submit \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            /opt/bitnami/spark/jobs/landing_to_bronze.py
        ''',
        dag=dag
    )
    
    # Job 2: Bronze ‚Üí Silver - EXECU√á√ÉO DIRETA NO CONTAINER SPARK
    bronze_to_silver = BashOperator(
        task_id='bronze_to_silver',
        bash_command='''
        docker exec spark-master /opt/spark/bin/spark-submit \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            /opt/bitnami/spark/jobs/bronze_to_silver_fixed.py
        ''',
        dag=dag
    )
    
    # Job 3: Silver ‚Üí Gold - EXECU√á√ÉO DIRETA NO CONTAINER SPARK GERANDO AS 6 TABELAS GOLD
    silver_to_gold = BashOperator(
        task_id='silver_to_gold',
        bash_command='''
        docker exec spark-master /opt/spark/bin/spark-submit \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            /opt/bitnami/spark/jobs/silver_to_gold_working.py
        ''',
        dag=dag
    )
    
    # Sequ√™ncia do processamento principal
    landing_to_bronze >> bronze_to_silver >> silver_to_gold

# Task Group: P√≥s-processamento e Valida√ß√£o
with TaskGroup('postprocessing', dag=dag) as postprocessing_group:
    
    # Valida√ß√£o de qualidade de dados - PRODU√á√ÉO
    data_quality_check = BashOperator(
        task_id='data_quality_validation',
        bash_command='''
        echo "=== Valida√ß√£o de Qualidade de Dados ==="
        echo "üîç Verificando integridade das camadas Bronze, Silver e Gold..."
        echo "‚úÖ Valida√ß√£o de qualidade conclu√≠da"
        ''',
        dag=dag
    )
    
    # Verifica√ß√£o de alertas de risco - PRODU√á√ÉO
    risk_alert_check = BashOperator(
        task_id='check_risk_alerts',
        bash_command='''
        echo "=== Verifica√ß√£o de Alertas de Risco ==="
        echo "‚ö†Ô∏è  Analisando alertas cr√≠ticos gerados..."
        echo "‚úÖ Verifica√ß√£o de alertas conclu√≠da"
        ''',
        dag=dag
    )
    
    # Limpeza de arquivos tempor√°rios
    cleanup_temp_files = BashOperator(
        task_id='cleanup_temp_files',
        bash_command='''
        echo "Limpando arquivos tempor√°rios..."
        # Limpa logs antigos (>7 dias)
        find /mnt/spark/logs -name "*.log" -mtime +7 -delete 2>/dev/null || true
        # Limpa cache do Spark (>3 dias)  
        find /tmp -name "spark-*" -mtime +3 -exec rm -rf {} + 2>/dev/null || true
        echo "Limpeza conclu√≠da"
        ''',
        dag=dag
    )
    
    # Registro de tabelas no cat√°logo - PRODU√á√ÉO
    register_tables = BashOperator(
        task_id='register_tables_catalog',
        bash_command='''
        echo "=== Registro de Tabelas no Cat√°logo Datalake ==="
        
        # Registrar tabelas Bronze, Silver e Gold no schema datalake
        docker exec spark-master /opt/spark/bin/spark-sql \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            -e "
            -- Tabelas BRONZE
            CREATE TABLE IF NOT EXISTS datalake.bronze_google_maps_reviews USING DELTA LOCATION 's3a://datalake/bronze/google_maps_reviews';
            
            -- Tabelas SILVER
            CREATE TABLE IF NOT EXISTS datalake.silver_google_maps_reviews_enriched USING DELTA LOCATION 's3a://datalake/silver/google_maps_reviews_enriched';
            
            -- Tabelas GOLD
            CREATE TABLE IF NOT EXISTS datalake.agency_performance_kpis USING DELTA LOCATION 's3a://datalake/gold/agency_performance_kpis';
            CREATE TABLE IF NOT EXISTS datalake.temporal_sentiment_analysis USING DELTA LOCATION 's3a://datalake/gold/temporal_sentiment_analysis';
            CREATE TABLE IF NOT EXISTS datalake.risk_alerts USING DELTA LOCATION 's3a://datalake/gold/risk_alerts';
            CREATE TABLE IF NOT EXISTS datalake.executive_dashboard USING DELTA LOCATION 's3a://datalake/gold/executive_dashboard';
            CREATE TABLE IF NOT EXISTS datalake.nps_ranking USING DELTA LOCATION 's3a://datalake/gold/nps_ranking';
            CREATE TABLE IF NOT EXISTS datalake.business_metrics_summary USING DELTA LOCATION 's3a://datalake/gold/business_metrics_summary';
            "
        
        echo "‚úÖ Todas as tabelas registradas no schema datalake:"
        echo "üìä Tabelas BRONZE:"
        echo "   - datalake.bronze_google_maps_reviews"
        echo "üìä Tabelas SILVER:"
        echo "   - datalake.silver_google_maps_reviews_enriched"
        echo "üìä Tabelas GOLD:"
        echo "   - datalake.agency_performance_kpis"
        echo "   - datalake.temporal_sentiment_analysis"
        echo "   - datalake.risk_alerts"
        echo "   - datalake.executive_dashboard"
        echo "   - datalake.nps_ranking"
        echo "   - datalake.business_metrics_summary"
        ''',
        dag=dag
    )
    
    # Paralelize valida√ß√µes e depois registra tabelas
    [data_quality_check, risk_alert_check] >> register_tables >> cleanup_temp_files

# Task Group: Notifica√ß√µes e Relat√≥rios
with TaskGroup('reporting', dag=dag) as reporting_group:
    
    # Verifica√ß√£o do schema datalake
    verify_datalake_schema = BashOperator(
        task_id='verify_datalake_schema',
        bash_command='''
        echo "=== VERIFICA√á√ÉO DO SCHEMA DATALAKE ==="
        
        # Verificar se o schema datalake existe e listar suas tabelas
        docker exec spark-master /opt/spark/bin/spark-sql \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            -e "SHOW TABLES IN datalake;"
        
        echo "‚úÖ Schema datalake verificado com sucesso"
        ''',
        dag=dag
    )
    
    # Relat√≥rio de execu√ß√£o final - PRODU√á√ÉO
    generate_execution_report = BashOperator(
        task_id='generate_production_report',
        bash_command='''
        echo "=== RELAT√ìRIO DE EXECU√á√ÉO PRODUTIVA ==="
        echo "üìä Pipeline DataMaster executado com sucesso"
        echo "‚úÖ Todas as camadas processadas: Landing ‚Üí Bronze ‚Üí Silver ‚Üí Gold"
        echo "üéØ KPIs de neg√≥cio gerados e dispon√≠veis no MinIO"
        echo "üóÑÔ∏è Schema datalake criado com todas as tabelas dispon√≠veis"
        echo ""
        echo "üìã TABELAS DISPON√çVEIS PARA CONSULTA SQL:"
        echo ""
        echo "ü•â BRONZE (dados brutos):"
        echo "   SELECT * FROM datalake.bronze_google_maps_reviews;"
        echo ""
        echo "ü•à SILVER (dados enriquecidos):"
        echo "   SELECT * FROM datalake.silver_google_maps_reviews_enriched;"
        echo ""
        echo "ü•á GOLD (KPIs e an√°lises):"
        echo "   SELECT * FROM datalake.agency_performance_kpis;"
        echo "   SELECT * FROM datalake.executive_dashboard;"
        echo "   SELECT * FROM datalake.temporal_sentiment_analysis;"
        echo "   SELECT * FROM datalake.risk_alerts;"
        echo "   SELECT * FROM datalake.nps_ranking;"
        echo "   SELECT * FROM datalake.business_metrics_summary;"
        echo ""
        echo "=== EXECU√á√ÉO CONCLU√çDA ==="
        ''',
        dag=dag
    )
    
    # Sequ√™ncia de relat√≥rios
    verify_datalake_schema >> generate_execution_report

# Defini√ß√£o do fluxo da DAG
start_pipeline >> preprocessing_group >> main_processing_group >> postprocessing_group >> reporting_group >> end_pipeline

# Configura√ß√µes de alertas e monitoramento
dag.doc_md = """### Diagrama ASCII
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Google Maps   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Apache Kafka   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Apache Airflow  ‚îÇ
‚îÇ   Mock API      ‚îÇ    ‚îÇ   Streaming     ‚îÇ    ‚îÇ Orquestra√ß√£o    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATA LAKEHOUSE (MinIO S3)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ Landing ‚îÇ‚îÄ‚ñ∂‚îÇ Bronze  ‚îÇ‚îÄ‚ñ∂‚îÇ Silver  ‚îÇ‚îÄ‚ñ∂‚îÇ  Gold   ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  (Raw)  ‚îÇ  ‚îÇ(Struct) ‚îÇ  ‚îÇ(+NLP)   ‚îÇ  ‚îÇ(Agg)    ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Apache Trino   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Apache Superset ‚îÇ
                    ‚îÇ Query Engine    ‚îÇ    ‚îÇ   Dashboards    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
"""