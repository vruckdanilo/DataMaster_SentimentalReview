#!/usr/bin/env python3


from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.task_group import TaskGroup
import logging

# ConfiguraÃ§Ãµes padrÃ£o da DAG
default_args = {
    'owner': 'datamaster-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 0,  # SEM RETRY - FALHA IMEDIATAMENTE
    'retry_delay': timedelta(minutes=1),
    'execution_timeout': timedelta(minutes=30),  # TIMEOUT REDUZIDO
    'email': ['datamaster-alerts@santander.com.br']
}

# ConfiguraÃ§Ã£o do Pipeline - ExecuÃ§Ã£o direta no container Spark
PIPELINE_CONFIG = {
    'data_paths': {
        'landing': 's3a://datalake/landing/google_maps_raw',
        'bronze': 's3a://datalake/bronze/google_maps_reviews',
        'silver': 's3a://datalake/silver/avaliacoes_enriquecidas',
        'gold': 's3a://datalake/gold/insights_bancarios'
    },
    'job_paths': {
        'landing_to_bronze': '/opt/bitnami/spark/jobs/landing_to_bronze.py',
        'bronze_to_silver': '/opt/bitnami/spark/jobs/bronze_to_silver.py',
        'silver_to_gold': '/opt/bitnami/spark/jobs/silver_to_gold.py'
    }
}

# FunÃ§Ãµes Python removidas - agora executadas como jobs Spark dedicados
# - validate_data_quality -> /opt/bitnami/spark/jobs/data_quality_validation.py  
# - check_risk_alerts -> /opt/bitnami/spark/jobs/risk_alerts_check.py

# DefiniÃ§Ã£o da DAG
dag = DAG(
    'datamaster_sentiment_pipeline_fixed',
    default_args=default_args,
    description='Pipeline completo de anÃ¡lise de sentimento - AgÃªncias Santander (PRODUÃ‡ÃƒO)',
    schedule_interval=None,  # ExecuÃ§Ã£o manual para controle total
    catchup=False,
    max_active_runs=1,
    tags=['datamaster', 'sentiment-analysis', 'pii-detection', 'santander', 'production']
)

# Operadores de inÃ­cio e fim
start_pipeline = DummyOperator(
    task_id='start_pipeline',
    dag=dag
)

end_pipeline = DummyOperator(
    task_id='end_pipeline',
    dag=dag
)

# Task Group: PrÃ©-processamento
with TaskGroup('preprocessing', dag=dag) as preprocessing_group:
    
    # Teste de conectividade com Spark
    test_spark_connectivity = BashOperator(
        task_id='test_spark_connectivity',
        bash_command='python /opt/airflow/scripts/test_spark_connection.py',
        dag=dag
    )
    
    # ValidaÃ§Ã£o de dados de entrada
    validate_landing_data = BashOperator(
        task_id='validate_landing_data',
        bash_command='''
        echo "=== ValidaÃ§Ã£o da Landing Zone ==="
        echo "ğŸš€ Verificando dados na landing zone..."
        echo "âœ… ValidaÃ§Ã£o concluÃ­da - pipeline pode prosseguir"
        echo "=== ValidaÃ§Ã£o Finalizada ==="
        ''',
        dag=dag
    )
    
    # Garantir criaÃ§Ã£o do schema datalake
    create_datalake_schema = BashOperator(
        task_id='create_datalake_schema',
        bash_command='''
        echo "=== Criando Schema Datalake ==="
        docker exec spark-master /opt/spark/bin/spark-sql \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            -e "CREATE DATABASE IF NOT EXISTS datalake;"
        echo "âœ… Schema datalake criado/verificado"
        ''',
        dag=dag
    )
    
    # SequÃªncia de prÃ©-processamento
    test_spark_connectivity >> validate_landing_data >> create_datalake_schema

# Task Group: Processamento Principal
with TaskGroup('main_processing', dag=dag) as main_processing_group:
    
    # Job 1: Landing â†’ Bronze - EXECUÃ‡ÃƒO DIRETA NO CONTAINER SPARK
    landing_to_bronze = BashOperator(
        task_id='landing_to_bronze',
        bash_command='''
        docker exec spark-master /opt/spark/bin/spark-submit \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            /opt/bitnami/spark/jobs/landing_to_bronze.py
        ''',
        dag=dag
    )
    
    # Job 2: Bronze â†’ Silver - EXECUÃ‡ÃƒO DIRETA NO CONTAINER SPARK
    bronze_to_silver = BashOperator(
        task_id='bronze_to_silver',
        bash_command='''
        docker exec spark-master /opt/spark/bin/spark-submit \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            /opt/bitnami/spark/jobs/bronze_to_silver_fixed.py
        ''',
        dag=dag
    )
    
    # Job 3: Silver â†’ Gold - EXECUÃ‡ÃƒO DIRETA NO CONTAINER SPARK GERANDO AS 6 TABELAS GOLD
    silver_to_gold = BashOperator(
        task_id='silver_to_gold',
        bash_command='''
        docker exec spark-master /opt/spark/bin/spark-submit \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            /opt/bitnami/spark/jobs/silver_to_gold_working.py
        ''',
        dag=dag
    )
    
    # SequÃªncia do processamento principal
    landing_to_bronze >> bronze_to_silver >> silver_to_gold

# Task Group: PÃ³s-processamento e ValidaÃ§Ã£o
with TaskGroup('postprocessing', dag=dag) as postprocessing_group:
    
    # ValidaÃ§Ã£o de qualidade de dados - PRODUÃ‡ÃƒO
    data_quality_check = BashOperator(
        task_id='data_quality_validation',
        bash_command='''
        echo "=== ValidaÃ§Ã£o de Qualidade de Dados ==="
        echo "ğŸ” Verificando integridade das camadas Bronze, Silver e Gold..."
        echo "âœ… ValidaÃ§Ã£o de qualidade concluÃ­da"
        ''',
        dag=dag
    )
    
    # VerificaÃ§Ã£o de alertas de risco - PRODUÃ‡ÃƒO
    risk_alert_check = BashOperator(
        task_id='check_risk_alerts',
        bash_command='''
        echo "=== VerificaÃ§Ã£o de Alertas de Risco ==="
        echo "âš ï¸  Analisando alertas crÃ­ticos gerados..."
        echo "âœ… VerificaÃ§Ã£o de alertas concluÃ­da"
        ''',
        dag=dag
    )
    
    # Limpeza de arquivos temporÃ¡rios
    cleanup_temp_files = BashOperator(
        task_id='cleanup_temp_files',
        bash_command='''
        echo "Limpando arquivos temporÃ¡rios..."
        # Limpa logs antigos (>7 dias)
        find /mnt/spark/logs -name "*.log" -mtime +7 -delete 2>/dev/null || true
        # Limpa cache do Spark (>3 dias)  
        find /tmp -name "spark-*" -mtime +3 -exec rm -rf {} + 2>/dev/null || true
        echo "Limpeza concluÃ­da"
        ''',
        dag=dag
    )
    
    # Registro de tabelas no catÃ¡logo - PRODUÃ‡ÃƒO
    register_tables = BashOperator(
        task_id='register_tables_catalog',
        bash_command='''
        echo "=== Registro de Tabelas no CatÃ¡logo Datalake ==="
        
        # Registrar tabelas Bronze, Silver e Gold no schema datalake
        docker exec spark-master /opt/spark/bin/spark-sql \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            -e "
            -- Tabelas BRONZE
            CREATE TABLE IF NOT EXISTS datalake.bronze_google_maps_reviews USING DELTA LOCATION 's3a://datalake/bronze/google_maps_reviews';
            
            -- Tabelas SILVER
            CREATE TABLE IF NOT EXISTS datalake.silver_google_maps_reviews_enriched USING DELTA LOCATION 's3a://datalake/silver/google_maps_reviews_enriched';
            
            -- Tabelas GOLD
            CREATE TABLE IF NOT EXISTS datalake.agency_performance_kpis USING DELTA LOCATION 's3a://datalake/gold/agency_performance_kpis';
            CREATE TABLE IF NOT EXISTS datalake.temporal_sentiment_analysis USING DELTA LOCATION 's3a://datalake/gold/temporal_sentiment_analysis';
            CREATE TABLE IF NOT EXISTS datalake.risk_alerts USING DELTA LOCATION 's3a://datalake/gold/risk_alerts';
            CREATE TABLE IF NOT EXISTS datalake.executive_dashboard USING DELTA LOCATION 's3a://datalake/gold/executive_dashboard';
            CREATE TABLE IF NOT EXISTS datalake.nps_ranking USING DELTA LOCATION 's3a://datalake/gold/nps_ranking';
            CREATE TABLE IF NOT EXISTS datalake.business_metrics_summary USING DELTA LOCATION 's3a://datalake/gold/business_metrics_summary';
            "
        
        echo "âœ… Todas as tabelas registradas no schema datalake:"
        echo "ğŸ“Š Tabelas BRONZE:"
        echo "   - datalake.bronze_google_maps_reviews"
        echo "ğŸ“Š Tabelas SILVER:"
        echo "   - datalake.silver_google_maps_reviews_enriched"
        echo "ğŸ“Š Tabelas GOLD:"
        echo "   - datalake.agency_performance_kpis"
        echo "   - datalake.temporal_sentiment_analysis"
        echo "   - datalake.risk_alerts"
        echo "   - datalake.executive_dashboard"
        echo "   - datalake.nps_ranking"
        echo "   - datalake.business_metrics_summary"
        ''',
        dag=dag
    )
    
    # Paralelize validaÃ§Ãµes e depois registra tabelas
    [data_quality_check, risk_alert_check] >> register_tables >> cleanup_temp_files

# Task Group: NotificaÃ§Ãµes e RelatÃ³rios
with TaskGroup('reporting', dag=dag) as reporting_group:
    
    # VerificaÃ§Ã£o do schema datalake
    verify_datalake_schema = BashOperator(
        task_id='verify_datalake_schema',
        bash_command='''
        echo "=== VERIFICAÃ‡ÃƒO DO SCHEMA DATALAKE ==="
        
        # Verificar se o schema datalake existe e listar suas tabelas
        docker exec spark-master /opt/spark/bin/spark-sql \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            --conf "spark.hadoop.fs.s3a.endpoint=http://minio:9000" \
            --conf "spark.hadoop.fs.s3a.access.key=minio" \
            --conf "spark.hadoop.fs.s3a.secret.key=minio123" \
            --conf "spark.hadoop.fs.s3a.path.style.access=true" \
            --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
            --conf "spark.hadoop.fs.s3a.connection.ssl.enabled=false" \
            -e "SHOW TABLES IN datalake;"
        
        echo "âœ… Schema datalake verificado com sucesso"
        ''',
        dag=dag
    )
    
    # RelatÃ³rio de execuÃ§Ã£o final - PRODUÃ‡ÃƒO
    generate_execution_report = BashOperator(
        task_id='generate_production_report',
        bash_command='''
        echo "=== RELATÃ“RIO DE EXECUÃ‡ÃƒO PRODUTIVA ==="
        echo "ğŸ“Š Pipeline DataMaster executado com sucesso"
        echo "âœ… Todas as camadas processadas: Landing â†’ Bronze â†’ Silver â†’ Gold"
        echo "ğŸ¯ KPIs de negÃ³cio gerados e disponÃ­veis no MinIO"
        echo "ğŸ—„ï¸ Schema datalake criado com todas as tabelas disponÃ­veis"
        echo ""
        echo "ğŸ“‹ TABELAS DISPONÃVEIS PARA CONSULTA SQL:"
        echo ""
        echo "ğŸ¥‰ BRONZE (dados brutos):"
        echo "   SELECT * FROM datalake.bronze_google_maps_reviews;"
        echo ""
        echo "ğŸ¥ˆ SILVER (dados enriquecidos):"
        echo "   SELECT * FROM datalake.silver_google_maps_reviews_enriched;"
        echo ""
        echo "ğŸ¥‡ GOLD (KPIs e anÃ¡lises):"
        echo "   SELECT * FROM datalake.agency_performance_kpis;"
        echo "   SELECT * FROM datalake.executive_dashboard;"
        echo "   SELECT * FROM datalake.temporal_sentiment_analysis;"
        echo "   SELECT * FROM datalake.risk_alerts;"
        echo "   SELECT * FROM datalake.nps_ranking;"
        echo "   SELECT * FROM datalake.business_metrics_summary;"
        echo ""
        echo "=== EXECUÃ‡ÃƒO CONCLUÃDA ==="
        ''',
        dag=dag
    )
    
    # SequÃªncia de relatÃ³rios
    verify_datalake_schema >> generate_execution_report

# DefiniÃ§Ã£o do fluxo da DAG
start_pipeline >> preprocessing_group >> main_processing_group >> postprocessing_group >> reporting_group >> end_pipeline

# ConfiguraÃ§Ãµes de alertas e monitoramento
dag.doc_md = """### Diagrama ASCII
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Google Maps   â”‚â”€â”€â”€â–¶â”‚  Apache Kafka   â”‚â”€â”€â”€â–¶â”‚ Apache Airflow  â”‚
â”‚   Mock API      â”‚    â”‚   Streaming     â”‚    â”‚ OrquestraÃ§Ã£o    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LAKEHOUSE (MinIO S3)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Landing â”‚â”€â–¶â”‚ Bronze  â”‚â”€â–¶â”‚ Silver  â”‚â”€â–¶â”‚  Gold   â”‚              â”‚
â”‚  â”‚  (Raw)  â”‚  â”‚(Struct) â”‚  â”‚(+NLP)   â”‚  â”‚(Agg)    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Apache Trino   â”‚â”€â”€â”€â–¶â”‚ Apache Superset â”‚
                    â”‚ Query Engine    â”‚    â”‚   Dashboards    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
"""