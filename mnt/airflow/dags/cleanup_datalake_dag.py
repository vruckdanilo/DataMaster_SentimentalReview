#!/usr/bin/env python3


from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
import logging

# Configura√ß√µes da DAG
default_args = {
    'owner': 'datamaster-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=1),
    'email': ['datamaster-alerts@santander.com.br']
}

def confirm_cleanup_operation(**context):
    """Fun√ß√£o de confirma√ß√£o antes da limpeza"""
    logger = logging.getLogger(__name__)
    
    logger.warning("=" * 60)
    logger.warning("‚ö†Ô∏è  ATEN√á√ÉO: OPERA√á√ÉO DE LIMPEZA COMPLETA")
    logger.warning("‚ö†Ô∏è  Esta DAG remove TODOS os dados do datalake!")
    logger.warning("‚ö†Ô∏è  - Tabelas do schema datalake")
    logger.warning("‚ö†Ô∏è  - Dados do MinIO (bucket datalake)")
    logger.warning("‚ö†Ô∏è  - Arquivos tempor√°rios e logs")
    logger.warning("=" * 60)
    logger.warning("üìã Para prosseguir, confirme que:")
    logger.warning("   1. Voc√™ tem certeza da opera√ß√£o")
    logger.warning("   2. Backup foi feito se necess√°rio")
    logger.warning("   3. Execu√ß√£o √© intencional")
    logger.warning("=" * 60)
    
    # Em produ√ß√£o, aqui poderia ter uma verifica√ß√£o adicional
    # como um par√¢metro de confirma√ß√£o ou token de seguran√ßa
    
    return True

# Defini√ß√£o da DAG
dag = DAG(
    'cleanup_datalake_complete',
    default_args=default_args,
    description='üßπ Limpeza completa do DataLake e MinIO - EXECU√á√ÉO MANUAL',
    schedule_interval=None,  # APENAS EXECU√á√ÉO MANUAL
    catchup=False,
    max_active_runs=1,
    tags=['cleanup', 'manual-only', 'datalake', 'minio', 'reset']
)

# Operadores de in√≠cio e fim
start_cleanup = DummyOperator(
    task_id='start_cleanup',
    dag=dag
)

end_cleanup = DummyOperator(
    task_id='end_cleanup',
    dag=dag
)

# Task Group: Confirma√ß√£o e Prepara√ß√£o
with TaskGroup('preparation', dag=dag) as preparation_group:
    
    # Confirma√ß√£o da opera√ß√£o
    confirm_operation = PythonOperator(
        task_id='confirm_cleanup_operation',
        python_callable=confirm_cleanup_operation,
        dag=dag
    )
    
    # Verifica√ß√£o de depend√™ncias
    check_dependencies = BashOperator(
        task_id='check_dependencies',
        bash_command='''
        echo "=== VERIFICANDO DEPEND√äNCIAS ==="
        echo "üîç Verificando conex√µes de rede..."
        
        # Verificar MinIO
        MINIO_STATUS=$(curl -s -o /dev/null -w '%{http_code}' http://minio:9000/minio/health/live 2>/dev/null || echo 'OFFLINE')
        echo "MinIO: $MINIO_STATUS"
        
        # Verificar Spark Master
        SPARK_STATUS=$(curl -s -o /dev/null -w '%{http_code}' http://spark-master:8082 2>/dev/null || echo 'OFFLINE')
        echo "Spark Master: $SPARK_STATUS"
        
        # Verificar Hive Metastore (porta 9083)
        HIVE_STATUS=$(nc -z hive-metastore 9083 2>/dev/null && echo "200" || echo "OFFLINE")
        echo "Hive Metastore: $HIVE_STATUS"
        
        echo ""
        echo "‚úÖ Verifica√ß√£o de depend√™ncias conclu√≠da"
        echo "‚ö†Ô∏è Prosseguindo com limpeza independente do status dos servi√ßos"
        ''',
        dag=dag
    )
    
    # Backup de metadados (opcional)
    backup_metadata = BashOperator(
        task_id='backup_metadata',
        bash_command='''
        echo "=== BACKUP DE METADADOS ==="
        mkdir -p /opt/airflow/backups/$(date +%Y%m%d_%H%M%S)
        echo "üìÅ Diret√≥rio de backup criado"
        
        # Backup de arquivos de status existentes
        if [ -d "/opt/airflow/spark_jobs/status" ]; then
            cp -r /opt/airflow/spark_jobs/status/* /opt/airflow/backups/$(date +%Y%m%d_%H%M%S)/ 2>/dev/null || true
            echo "üìã Arquivos de status copiados para backup"
        fi
        
        echo "‚úÖ Backup de metadados conclu√≠do"
        ''',
        dag=dag
    )
    
    # Sequ√™ncia de prepara√ß√£o
    confirm_operation >> check_dependencies >> backup_metadata

# Task Group: Limpeza do Schema/Cat√°logo
with TaskGroup('cleanup_catalog', dag=dag) as cleanup_catalog_group:
    
    # Limpeza completa do cat√°logo Hive - EXECU√á√ÉO DIRETA VIA SPARK
    cleanup_hive_catalog = BashOperator(
        task_id='cleanup_hive_catalog',
        bash_command='''
        echo "=== LIMPEZA DO CAT√ÅLOGO HIVE/DELTA ==="
        
        # Executar limpeza do cat√°logo usando comandos SQL diretos
        docker exec spark-master /opt/spark/bin/spark-sql \
            --packages io.delta:delta-core_2.12:2.4.0 \
            --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
            --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
            -e "DROP DATABASE IF EXISTS datalake CASCADE;" || echo "‚ö†Ô∏è Database datalake n√£o existe"
        
        echo "üóëÔ∏è Database datalake removido do cat√°logo"
        
        echo "‚úÖ Limpeza do cat√°logo conclu√≠da"
        ''',
        dag=dag
    )

# Task Group: Limpeza do MinIO
with TaskGroup('cleanup_storage', dag=dag) as cleanup_storage_group:
    
    # Limpeza dos buckets MinIO - EXECU√á√ÉO DIRETA VIA MC CLIENT
    cleanup_minio_buckets = BashOperator(
        task_id='cleanup_minio_buckets',
        bash_command='''
        echo "=== LIMPEZA DO MINIO DATALAKE ==="
        
        # Executar limpeza direta do MinIO usando Python/boto3 - vers√£o robusta
        python3 -c "
import boto3
from botocore.exceptions import ClientError

# Configurar cliente S3 para MinIO
s3_client = boto3.client(
    's3',
    endpoint_url='http://minio:9000',
    aws_access_key_id='minio',
    aws_secret_access_key='minio123',
    aws_session_token=None,
    region_name='us-east-1'
)

# Fun√ß√£o para limpar pasta recursivamente - vers√£o mais robusta
def clean_folder_complete(bucket, prefix):
    try:
        # Listar todos os objetos com pagina√ß√£o
        paginator = s3_client.get_paginator('list_objects_v2')
        page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)
        
        total_deleted = 0
        for page in page_iterator:
            if 'Contents' in page:
                objects = [{'Key': obj['Key']} for obj in page['Contents']]
                if objects:
                    # Deletar objetos em lotes
                    s3_client.delete_objects(Bucket=bucket, Delete={'Objects': objects})
                    total_deleted += len(objects)
        
        if total_deleted > 0:
            print(f'‚úÖ Pasta {prefix} limpa - {total_deleted} objetos removidos')
        else:
            print(f'‚ö†Ô∏è Pasta {prefix} j√° estava vazia')
    except ClientError as e:
        print(f'‚ö†Ô∏è Erro ao limpar {prefix}: {e}')

# Limpar todas as pastas incluindo subpastas
folders_to_clean = ['landing/', 'bronze/', 'silver/', 'gold/', 'controle/']
for folder in folders_to_clean:
    clean_folder_complete('datalake', folder)
"
        
        # Limpeza adicional via comando direto no filesystem
        echo "üóëÔ∏è Limpeza adicional via filesystem..."
        docker exec minio rm -rf /data/datalake/landing/* 2>/dev/null || true
        docker exec minio rm -rf /data/datalake/bronze/* 2>/dev/null || true
        docker exec minio rm -rf /data/datalake/silver/* 2>/dev/null || true
        docker exec minio rm -rf /data/datalake/gold/* 2>/dev/null || true
        docker exec minio rm -rf /data/datalake/controle/* 2>/dev/null || true
        
        echo "‚úÖ Limpeza do MinIO conclu√≠da"
        ''',
        dag=dag
    )

# Task Group: Limpeza de Arquivos Tempor√°rios
with TaskGroup('cleanup_temp_files', dag=dag) as cleanup_temp_group:
    
    # Limpeza de logs do Airflow
    cleanup_airflow_logs = BashOperator(
        task_id='cleanup_airflow_logs',
        bash_command='''
        echo "=== LIMPANDO LOGS DO AIRFLOW ==="
        
        # Remover logs antigos (> 7 dias)
        find /opt/airflow/logs -name "*.log" -mtime +7 -delete 2>/dev/null || true
        echo "üìÑ Logs antigos do Airflow removidos"
        
        # Limpar cache de tasks
        find /opt/airflow/logs -name "*.pid" -delete 2>/dev/null || true
        echo "üóÇÔ∏è Arquivos PID removidos"
        
        echo "‚úÖ Limpeza de logs do Airflow conclu√≠da"
        ''',
        dag=dag
    )
    
    # Limpeza de arquivos tempor√°rios do Spark
    cleanup_spark_temp = BashOperator(
        task_id='cleanup_spark_temp',
        bash_command='''
        echo "=== LIMPANDO ARQUIVOS TEMPOR√ÅRIOS DO SPARK ==="
        
        # Limpar arquivos tempor√°rios locais
        rm -rf /opt/airflow/spark_jobs/scripts/* 2>/dev/null || true
        rm -rf /opt/airflow/spark_jobs/status/* 2>/dev/null || true
        rm -rf /opt/airflow/spark_jobs/logs/* 2>/dev/null || true
        echo "üìÇ Arquivos de jobs removidos"
        
        # Limpar cache do sistema
        find /tmp -name "spark-*" -type d -mtime +1 -exec rm -rf {} + 2>/dev/null || true
        echo "üóÑÔ∏è Cache tempor√°rio limpo"
        
        echo "‚úÖ Limpeza de arquivos tempor√°rios conclu√≠da"
        ''',
        dag=dag
    )
    
    # Limpeza em paralelo
    [cleanup_airflow_logs, cleanup_spark_temp]

# Task Group: Verifica√ß√£o Final
with TaskGroup('verification', dag=dag) as verification_group:
    
    # Verifica√ß√£o de limpeza
    verify_cleanup = BashOperator(
        task_id='verify_cleanup_complete',
        bash_command='''
        echo "=== VERIFICA√á√ÉO FINAL DE LIMPEZA ==="
        echo ""
        
        echo "üîç Verificando schema datalake..."
        # Esta verifica√ß√£o ser√° feita pelo pr√≥prio job Spark
        echo "   ‚ûú Verifica√ß√£o delegada aos jobs Spark"
        echo ""
        
        echo "üîç Verificando MinIO..."
        # Tentar listar objetos no bucket (deve estar vazio)
        echo "   ‚ûú Verifica√ß√£o delegada ao job MinIO"
        echo ""
        
        echo "üîç Verificando arquivos tempor√°rios..."
        TEMP_COUNT=$(find /opt/airflow/spark_jobs -name "*.json" 2>/dev/null | wc -l)
        echo "   ‚ûú Arquivos tempor√°rios restantes: $TEMP_COUNT"
        echo ""
        
        echo "‚úÖ Verifica√ß√£o final conclu√≠da"
        echo ""
        echo "üìã LIMPEZA COMPLETA FINALIZADA!"
        echo "   üßπ Schema datalake limpo"
        echo "   üßπ Dados MinIO removidos" 
        echo "   üßπ Arquivos tempor√°rios limpos"
        echo ""
        echo "üöÄ Ambiente pronto para execu√ß√£o limpa do pipeline!"
        ''',
        dag=dag
    )
    
    # Gera√ß√£o de relat√≥rio final
    generate_cleanup_report = BashOperator(
        task_id='generate_cleanup_report',
        bash_command='''
        echo "=== GERANDO RELAT√ìRIO DE LIMPEZA ==="
        
        REPORT_FILE="/tmp/cleanup_report_$(date +%Y%m%d_%H%M%S).txt"
        
        cat > "$REPORT_FILE" << EOF
====================================================================
RELAT√ìRIO DE LIMPEZA COMPLETA - DATAMASTER
====================================================================

Data/Hora: $(date)
DAG: cleanup_datalake_complete
Executado por: ${AIRFLOW_CTX_DAG_OWNER}

OPERA√á√ïES REALIZADAS:
‚úÖ Limpeza do schema datalake (Hive Metastore)
‚úÖ Limpeza dos dados MinIO (bucket datalake) 
‚úÖ Limpeza de arquivos tempor√°rios
‚úÖ Limpeza de logs antigos
‚úÖ Verifica√ß√£o final de integridade

STATUS: LIMPEZA COMPLETA REALIZADA

PR√ìXIMOS PASSOS:
1. Execute a DAG 'datamaster_sentiment_pipeline_fixed'
2. Aguarde conclus√£o completa do pipeline
3. Verifique se tabelas aparecem no schema datalake
4. Monitore logs para garantir execu√ß√£o limpa

====================================================================
EOF

        echo "üìÑ Relat√≥rio gerado: $REPORT_FILE"
        cat "$REPORT_FILE"
        
        # Copiar para √°rea de relat√≥rios se existir
        mkdir -p /opt/airflow/reports 2>/dev/null || true
        cp "$REPORT_FILE" /opt/airflow/reports/ 2>/dev/null || true
        
        echo "‚úÖ Relat√≥rio de limpeza conclu√≠do"
        ''',
        dag=dag
    )
    
    # Sequ√™ncia de verifica√ß√£o
    verify_cleanup >> generate_cleanup_report

# Defini√ß√£o do fluxo da DAG
start_cleanup >> preparation_group >> [cleanup_catalog_group, cleanup_storage_group] >> cleanup_temp_group >> verification_group >> end_cleanup

# Documenta√ß√£o da DAG
dag.doc_md = """
# DAG de Limpeza Completa - DataLake e MinIO

Esta DAG realiza limpeza completa do ambiente DataMaster para permitir execu√ß√£o limpa do pipeline.

## ‚ö†Ô∏è ATEN√á√ÉO - EXECU√á√ÉO MANUAL APENAS

**Esta DAG remove TODOS os dados permanentemente!**

### Opera√ß√µes Realizadas:

1. **Prepara√ß√£o**:
   - Confirma√ß√£o da opera√ß√£o
   - Verifica√ß√£o de depend√™ncias
   - Backup de metadados

2. **Limpeza do Cat√°logo**:
   - Remove todas as tabelas do schema `datalake`
   - Remove database `datalake` do Hive Metastore
   - Limpa registros de metadados

3. **Limpeza do Storage**:
   - Remove todos os objetos do bucket `datalake` no MinIO
   - Limpa arquivos Delta Lake (_delta_log)
   - Remove checkpoints e arquivos tempor√°rios

4. **Limpeza de Arquivos Tempor√°rios**:
   - Remove logs antigos do Airflow
   - Limpa arquivos tempor√°rios do Spark
   - Remove cache e arquivos PID

5. **Verifica√ß√£o Final**:
   - Verifica se limpeza foi completa
   - Gera relat√≥rio de opera√ß√µes
   - Confirma estado limpo do ambiente

## Quando Usar:

- **Reset completo** do ambiente
- **Problemas de corrup√ß√£o** de dados
- **Testes de pipeline** completo
- **Limpeza antes de deploy** em produ√ß√£o

## Como Executar:

1. **Via Airflow UI**: Trigger manual da DAG
2. **Via CLI**: `airflow dags trigger cleanup_datalake_complete`

## Ap√≥s a Execu√ß√£o:

1. Execute `datamaster_sentiment_pipeline_fixed`
2. Aguarde conclus√£o completa
3. Verifique tabelas em `datalake` schema
4. Monitore logs para garantir sucesso

## Backup:

A DAG faz backup autom√°tico de metadados em `/opt/airflow/backups/`
"""